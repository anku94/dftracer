stages:
- generate_data
- train
- compress_output
- move
- compact
- compress_final
- cleanup

variables:
  GIT_SUBMODULE_STRATEGY: recursive
  CUSTOM_CI_BUILDS_DIR: ''
  CUSTOM_CI_ENV_DIR: ''
  CUSTOM_CI_OUTPUT_DIR: /usr/workspace/haridev/gitlab-runner-outputs
  DLIO_BENCHMARK_REPO: ''
  DLIO_BENCHMARK_TAG: ''
  DFTRACER_REPO: ''
  ENV_NAME: ''
  PYTHON_MODULE: ''
  MPI_MODULE: ''
  GCC_MODULE: ''
  DATA_PATH: /p/lustre3/770529USER/gitlab-runner-data/
  LOG_STORE_DIR: /p/lustre3/iopp/dftracer-traces-lfs
  CORES: '48'
  GPUS: '8'
  NODES: '1'
  WALLTIME: '60'
  QUEUE: pdebug
  MAX_NODES: '8'
  SYSTEM_NAME: corona
.lc:
  id_tokens:
    SITE_ID_TOKEN:
      aud: https://lc.llnl.gov/gitlab
.corona:
  extends: .lc
  tags:
  - shell
  - corona
workload_1_generate_data:
  stage: generate_data
  extends: .corona
  script: |-
    ./variables.sh
    ./pre.sh
    if [ -d /p/lustre3/770529USER/gitlab-runner-data//bert_v100-1/ ]; then echo 'Directory /p/lustre3/770529USER/gitlab-runner-data//bert_v100-1/ already exists. Skipping data generation.'; else flux submit -N 1 --tasks-per-node=48 -q pdebug -t 60 --exclusive dlio_benchmark workload=bert_v100 ++workload.dataset.data_folder=/p/lustre3/770529USER/gitlab-runner-data//bert_v100-1//data ++workload.checkpoint.checkpoint_folder=/p/lustre3/770529USER/gitlab-runner-data//bert_v100-1//checkpoint ++workload.train.epochs=1 ++workload.output.folder=/usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/generate ++workload.workflow.generate_data=True ++workload.workflow.train=False; fi
    if [ -d /p/lustre3/770529USER/gitlab-runner-data//bert_v100-1/ ] && grep -i 'error' /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/generate/dlio.log; then echo 'Error found in dlio.log'; exit 1; fi
    last_job_id=$(flux job last | awk '{print $1}')
    echo 'Waiting for job ID: $last_job_id to finish...'
    flux job wait $last_job_id
workload_1_train:
  stage: train
  extends: .corona
  script:
  - ./variables.sh
  - ./pre.sh
  - flux submit -N 1 --tasks-per-node=8 -q pdebug -t 60 --exclusive dlio_benchmark
    workload=bert_v100 ++workload.dataset.data_folder=/p/lustre3/770529USER/gitlab-runner-data//bert_v100-1//data
    ++workload.checkpoint.checkpoint_folder=/p/lustre3/770529USER/gitlab-runner-data//bert_v100-1//checkpoint
    ++workload.train.epochs=1 ++workload.output.folder=/usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train
    hydra.run.dir=/usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - last_job_id=$(flux job last | awk '{print $1}')
  - 'echo ''Waiting for job ID: $last_job_id to finish...'''
  - flux job wait $last_job_id
  - if grep -i 'error' /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  needs:
  - workload_1_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
workload_1_compress_output:
  stage: compress_output
  extends: .corona
  script:
  - ./variables.sh
  - ./pre.sh
  - flux submit -N 1 --tasks-per-node=48 -q pdebug -t 60 --exclusive dftracer_pgzip
    -d /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train
  - last_job_id=$(flux job last | awk '{print $1}')
  - 'echo ''Waiting for job ID: $last_job_id to finish...'''
  - flux job wait $last_job_id
  - if find /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train
    -type f -name '*.pfw' | grep -q .; then echo 'Uncompressed .pfw files found!';
    exit 1; fi
  - if ! find /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train
    -type f -name '*.pfw.gz' | grep -q .; then echo 'No compressed .pfw.gz files found!';
    exit 1; fi
  needs:
  - workload_1_train
workload_1_move:
  stage: move
  extends: .corona
  script:
  - ./variables.sh
  - ./pre.sh
  - mkdir -p /p/lustre3/iopp/dftracer-traces-lfs/1.0.10.dev22/corona/bert_v100/nodes-1/33869546/RAW/
  - mv /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train/*.pfw.gz
    /p/lustre3/iopp/dftracer-traces-lfs/1.0.10.dev22/corona/bert_v100/nodes-1/33869546/RAW/
  - mv /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train/.hydra
    /p/lustre3/iopp/dftracer-traces-lfs/1.0.10.dev22/corona/bert_v100/nodes-1/33869546/
  - mv /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546/train/dlio.log
    /p/lustre3/iopp/dftracer-traces-lfs/1.0.10.dev22/corona/bert_v100/nodes-1/33869546/
  needs:
  - workload_1_compress_output
workload_1_compact:
  stage: compact
  extends: .corona
  script:
  - ./variables.sh
  - ./pre.sh
  - cd /p/lustre3/iopp/dftracer-traces-lfs/1.0.10.dev22/corona/bert_v100/nodes-1/33869546
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n bert_v100
  needs:
  - workload_1_move
workload_1_compress_final:
  stage: compress_final
  extends: .corona
  script:
  - ./variables.sh
  - ./pre.sh
  - cd /p/lustre3/iopp/dftracer-traces-lfs/1.0.10.dev22/corona/bert_v100/nodes-1/33869546
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - workload_1_compact
workload_1_cleanup:
  stage: cleanup
  extends: .corona
  script:
  - ./variables.sh
  - ./pre.sh
  - module load mpifileutils
  - flux submit -N 1 --tasks-per-node=48 -q pdebug -t 60 --exclusive drm /usr/workspace/haridev/gitlab-runner-outputs/bert_v100/33869546
  - last_job_id=$(flux job last | awk '{print $1}')
  - 'echo ''Waiting for job ID: $last_job_id to finish...'''
  - flux job wait $last_job_id
  needs:
  - workload_1_compress_final
