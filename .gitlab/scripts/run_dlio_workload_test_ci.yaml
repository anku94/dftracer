variables: {}
stages:
- create_directory
- generate_data
- train
- compress_output
- move
- compact
- cleanup_compact
- compress_final
- cleanup
include:
- project: lc-templates/id_tokens
  file: id_tokens.yml
- local: .gitlab/scripts/common.yml
create_directory_common:
  stage: create_directory
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - ./.gitlab/scripts/create_log_dir.sh
  needs: []
bert_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/bert/ ] && [ ! -f /tmp/bert//success ]; then echo 'Directory /tmp/bert/
    exist but is not complete.'; flux run -N 1 --tasks-per-node=1 -q squeue -t 60
    --exclusive drm /tmp/bert/;  fi
  - if [ -f /tmp/bert//success ]; then echo 'Directory /tmp/bert/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    60 --exclusive dlio_benchmark workload=bert_v100 ++workload.dataset.data_folder=/tmp/bert//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/bert_v100/1/20250327140733/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/bert/ ] && grep -i 'error' /tmp/bert_v100/1/20250327140733/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/bert//success
  needs:
  - create_directory_common
bert_v100_1_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=bert_v100
    ++workload.dataset.data_folder=/tmp/bert//data ++workload.checkpoint.checkpoint_folder=/tmp/bert_v100-1-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/bert_v100/1/20250327140733/train
    hydra.run.dir=/tmp/bert_v100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/bert_v100/1/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - bert_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
bert_v100_1_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/bert_v100/1/20250327140733/train
  - if find /tmp/bert_v100/1/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/bert_v100/1/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_1_train
bert_v100_1_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/bert_v100/nodes-1/20250327140733/RAW/
  - mv /tmp/bert_v100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/bert_v100/nodes-1/20250327140733/RAW/
  - mv /tmp/bert_v100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/bert_v100/nodes-1/20250327140733/
  - mv /tmp/bert_v100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/bert_v100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - bert_v100_1_1_compress_output
bert_v100_1_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n bert_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_1_move
bert_v100_1_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/bert_v100/nodes-1/20250327140733/COMPACT
  needs:
  - bert_v100_1_1_compact
  when: on_failure
bert_v100_1_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - bert_v100_1_1_move
  when: on_success
bert_v100_1_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/bert_v100/1/20250327140733
  needs:
    job: bert_v100_1_1_compress_final
    optional: true
  when: always
bert_v100_1_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=bert_v100
    ++workload.dataset.data_folder=/tmp/bert//data ++workload.checkpoint.checkpoint_folder=/tmp/bert_v100-1-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/bert_v100/2/20250327140733/train
    hydra.run.dir=/tmp/bert_v100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/bert_v100/2/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - bert_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
bert_v100_1_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/bert_v100/2/20250327140733/train
  - if find /tmp/bert_v100/2/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/bert_v100/2/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_2_train
bert_v100_1_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/bert_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/bert_v100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/bert_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/bert_v100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/bert_v100/nodes-2/20250327140733/
  - mv /tmp/bert_v100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/bert_v100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - bert_v100_1_2_compress_output
bert_v100_1_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n bert_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_2_move
bert_v100_1_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/bert_v100/nodes-2/20250327140733/COMPACT
  needs:
  - bert_v100_1_2_compact
  when: on_failure
bert_v100_1_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - bert_v100_1_2_move
  when: on_success
bert_v100_1_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/bert_v100/2/20250327140733
  needs:
    job: bert_v100_1_2_compress_final
    optional: true
  when: always
bert_v100_1_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=bert_v100
    ++workload.dataset.data_folder=/tmp/bert//data ++workload.checkpoint.checkpoint_folder=/tmp/bert_v100-1-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/bert_v100/4/20250327140733/train
    hydra.run.dir=/tmp/bert_v100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/bert_v100/4/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - bert_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
bert_v100_1_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/bert_v100/4/20250327140733/train
  - if find /tmp/bert_v100/4/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/bert_v100/4/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_4_train
bert_v100_1_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/bert_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/bert_v100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/bert_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/bert_v100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/bert_v100/nodes-4/20250327140733/
  - mv /tmp/bert_v100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/bert_v100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - bert_v100_1_4_compress_output
bert_v100_1_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n bert_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_4_move
bert_v100_1_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/bert_v100/nodes-4/20250327140733/COMPACT
  needs:
  - bert_v100_1_4_compact
  when: on_failure
bert_v100_1_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - bert_v100_1_4_move
  when: on_success
bert_v100_1_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/bert_v100/4/20250327140733
  needs:
    job: bert_v100_1_4_compress_final
    optional: true
  when: always
bert_v100_1_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=bert_v100
    ++workload.dataset.data_folder=/tmp/bert//data ++workload.checkpoint.checkpoint_folder=/tmp/bert_v100-1-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/bert_v100/8/20250327140733/train
    hydra.run.dir=/tmp/bert_v100/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/bert_v100/8/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - bert_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
bert_v100_1_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/bert_v100/8/20250327140733/train
  - if find /tmp/bert_v100/8/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/bert_v100/8/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_8_train
bert_v100_1_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/bert_v100/nodes-8/20250327140733/RAW/
  - mv /tmp/bert_v100/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/bert_v100/nodes-8/20250327140733/RAW/
  - mv /tmp/bert_v100/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/bert_v100/nodes-8/20250327140733/
  - mv /tmp/bert_v100/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/bert_v100/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - bert_v100_1_8_compress_output
bert_v100_1_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n bert_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_8_move
bert_v100_1_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/bert_v100/nodes-8/20250327140733/COMPACT
  needs:
  - bert_v100_1_8_compact
  when: on_failure
bert_v100_1_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/bert_v100/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - bert_v100_1_8_move
  when: on_success
bert_v100_1_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/bert_v100/8/20250327140733
  needs:
    job: bert_v100_1_8_compress_final
    optional: true
  when: always
cosmoflow_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/cosmoflow/ ] && [ ! -f /tmp/cosmoflow//success ]; then echo 'Directory
    /tmp/cosmoflow/ exist but is not complete.'; flux run -N 1 --tasks-per-node=1
    -q squeue -t 60 --exclusive drm /tmp/cosmoflow/;  fi
  - if [ -f /tmp/cosmoflow//success ]; then echo 'Directory /tmp/cosmoflow/ already
    exists. Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue
    -t 60 --exclusive dlio_benchmark workload=cosmoflow_a100 ++workload.dataset.data_folder=/tmp/cosmoflow//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_a100/1/20250327140733/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/cosmoflow/ ] && grep -i 'error' /tmp/cosmoflow_a100/1/20250327140733/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/cosmoflow//success
  needs:
  - create_directory_common
cosmoflow_a100_2_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_a100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_a100-2-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_a100/1/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_a100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_a100/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_a100_2_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_a100/1/20250327140733/train
  - if find /tmp/cosmoflow_a100/1/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_a100/1/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_1_train
cosmoflow_a100_2_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-1/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-1/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-1/20250327140733/
  - mv /tmp/cosmoflow_a100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_a100_2_1_compress_output
cosmoflow_a100_2_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_1_move
cosmoflow_a100_2_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-1/20250327140733/COMPACT
  needs:
  - cosmoflow_a100_2_1_compact
  when: on_failure
cosmoflow_a100_2_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_a100_2_1_move
  when: on_success
cosmoflow_a100_2_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_a100/1/20250327140733
  needs:
    job: cosmoflow_a100_2_1_compress_final
    optional: true
  when: always
cosmoflow_a100_2_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_a100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_a100-2-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_a100/2/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_a100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_a100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_a100_2_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_a100/2/20250327140733/train
  - if find /tmp/cosmoflow_a100/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_a100/2/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_2_train
cosmoflow_a100_2_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-2/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-2/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-2/20250327140733/
  - mv /tmp/cosmoflow_a100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_a100_2_2_compress_output
cosmoflow_a100_2_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_2_move
cosmoflow_a100_2_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-2/20250327140733/COMPACT
  needs:
  - cosmoflow_a100_2_2_compact
  when: on_failure
cosmoflow_a100_2_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_a100_2_2_move
  when: on_success
cosmoflow_a100_2_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_a100/2/20250327140733
  needs:
    job: cosmoflow_a100_2_2_compress_final
    optional: true
  when: always
cosmoflow_a100_2_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_a100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_a100-2-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_a100/4/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_a100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_a100/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_a100_2_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_a100/4/20250327140733/train
  - if find /tmp/cosmoflow_a100/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_a100/4/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_4_train
cosmoflow_a100_2_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-4/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-4/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-4/20250327140733/
  - mv /tmp/cosmoflow_a100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_a100_2_4_compress_output
cosmoflow_a100_2_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_4_move
cosmoflow_a100_2_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-4/20250327140733/COMPACT
  needs:
  - cosmoflow_a100_2_4_compact
  when: on_failure
cosmoflow_a100_2_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_a100_2_4_move
  when: on_success
cosmoflow_a100_2_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_a100/4/20250327140733
  needs:
    job: cosmoflow_a100_2_4_compress_final
    optional: true
  when: always
cosmoflow_a100_2_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_a100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_a100-2-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_a100/8/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_a100/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_a100/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_a100_2_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_a100/8/20250327140733/train
  - if find /tmp/cosmoflow_a100/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_a100/8/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_8_train
cosmoflow_a100_2_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-8/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-8/20250327140733/RAW/
  - mv /tmp/cosmoflow_a100/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-8/20250327140733/
  - mv /tmp/cosmoflow_a100/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_a100_2_8_compress_output
cosmoflow_a100_2_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_8_move
cosmoflow_a100_2_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-8/20250327140733/COMPACT
  needs:
  - cosmoflow_a100_2_8_compact
  when: on_failure
cosmoflow_a100_2_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_a100/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_a100_2_8_move
  when: on_success
cosmoflow_a100_2_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_a100/8/20250327140733
  needs:
    job: cosmoflow_a100_2_8_compress_final
    optional: true
  when: always
cosmoflow_h100_3_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_h100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_h100-3-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_h100/1/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_h100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_h100/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_h100_3_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_h100/1/20250327140733/train
  - if find /tmp/cosmoflow_h100/1/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_h100/1/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_1_train
cosmoflow_h100_3_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-1/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-1/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-1/20250327140733/
  - mv /tmp/cosmoflow_h100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_h100_3_1_compress_output
cosmoflow_h100_3_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_1_move
cosmoflow_h100_3_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-1/20250327140733/COMPACT
  needs:
  - cosmoflow_h100_3_1_compact
  when: on_failure
cosmoflow_h100_3_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_h100_3_1_move
  when: on_success
cosmoflow_h100_3_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_h100/1/20250327140733
  needs:
    job: cosmoflow_h100_3_1_compress_final
    optional: true
  when: always
cosmoflow_h100_3_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_h100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_h100-3-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_h100/2/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_h100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_h100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_h100_3_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_h100/2/20250327140733/train
  - if find /tmp/cosmoflow_h100/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_h100/2/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_2_train
cosmoflow_h100_3_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-2/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-2/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-2/20250327140733/
  - mv /tmp/cosmoflow_h100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_h100_3_2_compress_output
cosmoflow_h100_3_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_2_move
cosmoflow_h100_3_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-2/20250327140733/COMPACT
  needs:
  - cosmoflow_h100_3_2_compact
  when: on_failure
cosmoflow_h100_3_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_h100_3_2_move
  when: on_success
cosmoflow_h100_3_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_h100/2/20250327140733
  needs:
    job: cosmoflow_h100_3_2_compress_final
    optional: true
  when: always
cosmoflow_h100_3_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_h100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_h100-3-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_h100/4/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_h100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_h100/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_h100_3_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_h100/4/20250327140733/train
  - if find /tmp/cosmoflow_h100/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_h100/4/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_4_train
cosmoflow_h100_3_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-4/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-4/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-4/20250327140733/
  - mv /tmp/cosmoflow_h100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_h100_3_4_compress_output
cosmoflow_h100_3_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_4_move
cosmoflow_h100_3_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-4/20250327140733/COMPACT
  needs:
  - cosmoflow_h100_3_4_compact
  when: on_failure
cosmoflow_h100_3_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_h100_3_4_move
  when: on_success
cosmoflow_h100_3_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_h100/4/20250327140733
  needs:
    job: cosmoflow_h100_3_4_compress_final
    optional: true
  when: always
cosmoflow_h100_3_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_h100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_h100-3-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_h100/8/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_h100/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_h100/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_h100_3_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_h100/8/20250327140733/train
  - if find /tmp/cosmoflow_h100/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_h100/8/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_8_train
cosmoflow_h100_3_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-8/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-8/20250327140733/RAW/
  - mv /tmp/cosmoflow_h100/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-8/20250327140733/
  - mv /tmp/cosmoflow_h100/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_h100_3_8_compress_output
cosmoflow_h100_3_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_8_move
cosmoflow_h100_3_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-8/20250327140733/COMPACT
  needs:
  - cosmoflow_h100_3_8_compact
  when: on_failure
cosmoflow_h100_3_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_h100/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_h100_3_8_move
  when: on_success
cosmoflow_h100_3_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_h100/8/20250327140733
  needs:
    job: cosmoflow_h100_3_8_compress_final
    optional: true
  when: always
cosmoflow_v100_4_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_v100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_v100-4-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_v100/2/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_v100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_v100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_v100_4_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_v100/2/20250327140733/train
  - if find /tmp/cosmoflow_v100/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_v100/2/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_2_train
cosmoflow_v100_4_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/cosmoflow_v100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/cosmoflow_v100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-2/20250327140733/
  - mv /tmp/cosmoflow_v100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_v100_4_2_compress_output
cosmoflow_v100_4_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_2_move
cosmoflow_v100_4_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-2/20250327140733/COMPACT
  needs:
  - cosmoflow_v100_4_2_compact
  when: on_failure
cosmoflow_v100_4_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_v100_4_2_move
  when: on_success
cosmoflow_v100_4_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_v100/2/20250327140733
  needs:
    job: cosmoflow_v100_4_2_compress_final
    optional: true
  when: always
cosmoflow_v100_4_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_v100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_v100-4-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_v100/4/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_v100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_v100/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_v100_4_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_v100/4/20250327140733/train
  - if find /tmp/cosmoflow_v100/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_v100/4/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_4_train
cosmoflow_v100_4_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/cosmoflow_v100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/cosmoflow_v100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-4/20250327140733/
  - mv /tmp/cosmoflow_v100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_v100_4_4_compress_output
cosmoflow_v100_4_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_4_move
cosmoflow_v100_4_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-4/20250327140733/COMPACT
  needs:
  - cosmoflow_v100_4_4_compact
  when: on_failure
cosmoflow_v100_4_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_v100_4_4_move
  when: on_success
cosmoflow_v100_4_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_v100/4/20250327140733
  needs:
    job: cosmoflow_v100_4_4_compress_final
    optional: true
  when: always
cosmoflow_v100_4_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=cosmoflow_v100
    ++workload.dataset.data_folder=/tmp/cosmoflow//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_v100-4-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/cosmoflow_v100/8/20250327140733/train
    hydra.run.dir=/tmp/cosmoflow_v100/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_v100/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_v100_4_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_v100/8/20250327140733/train
  - if find /tmp/cosmoflow_v100/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_v100/8/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_8_train
cosmoflow_v100_4_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-8/20250327140733/RAW/
  - mv /tmp/cosmoflow_v100/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-8/20250327140733/RAW/
  - mv /tmp/cosmoflow_v100/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-8/20250327140733/
  - mv /tmp/cosmoflow_v100/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - cosmoflow_v100_4_8_compress_output
cosmoflow_v100_4_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_8_move
cosmoflow_v100_4_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-8/20250327140733/COMPACT
  needs:
  - cosmoflow_v100_4_8_compact
  when: on_failure
cosmoflow_v100_4_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/cosmoflow_v100/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - cosmoflow_v100_4_8_move
  when: on_success
cosmoflow_v100_4_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/cosmoflow_v100/8/20250327140733
  needs:
    job: cosmoflow_v100_4_8_compress_final
    optional: true
  when: always
dlrm_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/dlrm/ ] && [ ! -f /tmp/dlrm//success ]; then echo 'Directory /tmp/dlrm/
    exist but is not complete.'; flux run -N 1 --tasks-per-node=1 -q squeue -t 60
    --exclusive drm /tmp/dlrm/;  fi
  - if [ -f /tmp/dlrm//success ]; then echo 'Directory /tmp/dlrm/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    60 --exclusive dlio_benchmark workload=dlrm ++workload.dataset.data_folder=/tmp/dlrm//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/dlrm/1/20250327140733/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/dlrm/ ] && grep -i 'error' /tmp/dlrm/1/20250327140733/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/dlrm//success
  needs:
  - create_directory_common
dlrm_6_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=dlrm
    ++workload.dataset.data_folder=/tmp/dlrm//data ++workload.checkpoint.checkpoint_folder=/tmp/dlrm-6-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/dlrm/1/20250327140733/train
    hydra.run.dir=/tmp/dlrm/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/dlrm/1/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - dlrm_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
dlrm_6_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/dlrm/1/20250327140733/train
  - if find /tmp/dlrm/1/20250327140733/train -type f -name '*.pfw' | grep -q .; then
    echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/dlrm/1/20250327140733/train -type f -name '*.pfw.gz' | grep -q
    .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_1_train
dlrm_6_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/dlrm/nodes-1/20250327140733/RAW/
  - mv /tmp/dlrm/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/dlrm/nodes-1/20250327140733/RAW/
  - mv /tmp/dlrm/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/dlrm/nodes-1/20250327140733/
  - mv /tmp/dlrm/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/dlrm/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - dlrm_6_1_compress_output
dlrm_6_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n dlrm
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_1_move
dlrm_6_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/dlrm/nodes-1/20250327140733/COMPACT
  needs:
  - dlrm_6_1_compact
  when: on_failure
dlrm_6_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - dlrm_6_1_move
  when: on_success
dlrm_6_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/dlrm/1/20250327140733
  needs:
    job: dlrm_6_1_compress_final
    optional: true
  when: always
dlrm_6_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=dlrm
    ++workload.dataset.data_folder=/tmp/dlrm//data ++workload.checkpoint.checkpoint_folder=/tmp/dlrm-6-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/dlrm/2/20250327140733/train
    hydra.run.dir=/tmp/dlrm/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/dlrm/2/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - dlrm_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
dlrm_6_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/dlrm/2/20250327140733/train
  - if find /tmp/dlrm/2/20250327140733/train -type f -name '*.pfw' | grep -q .; then
    echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/dlrm/2/20250327140733/train -type f -name '*.pfw.gz' | grep -q
    .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_2_train
dlrm_6_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/dlrm/nodes-2/20250327140733/RAW/
  - mv /tmp/dlrm/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/dlrm/nodes-2/20250327140733/RAW/
  - mv /tmp/dlrm/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/dlrm/nodes-2/20250327140733/
  - mv /tmp/dlrm/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/dlrm/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - dlrm_6_2_compress_output
dlrm_6_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n dlrm
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_2_move
dlrm_6_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/dlrm/nodes-2/20250327140733/COMPACT
  needs:
  - dlrm_6_2_compact
  when: on_failure
dlrm_6_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - dlrm_6_2_move
  when: on_success
dlrm_6_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/dlrm/2/20250327140733
  needs:
    job: dlrm_6_2_compress_final
    optional: true
  when: always
dlrm_6_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=dlrm
    ++workload.dataset.data_folder=/tmp/dlrm//data ++workload.checkpoint.checkpoint_folder=/tmp/dlrm-6-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/dlrm/4/20250327140733/train
    hydra.run.dir=/tmp/dlrm/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/dlrm/4/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - dlrm_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
dlrm_6_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/dlrm/4/20250327140733/train
  - if find /tmp/dlrm/4/20250327140733/train -type f -name '*.pfw' | grep -q .; then
    echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/dlrm/4/20250327140733/train -type f -name '*.pfw.gz' | grep -q
    .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_4_train
dlrm_6_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/dlrm/nodes-4/20250327140733/RAW/
  - mv /tmp/dlrm/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/dlrm/nodes-4/20250327140733/RAW/
  - mv /tmp/dlrm/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/dlrm/nodes-4/20250327140733/
  - mv /tmp/dlrm/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/dlrm/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - dlrm_6_4_compress_output
dlrm_6_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n dlrm
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_4_move
dlrm_6_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/dlrm/nodes-4/20250327140733/COMPACT
  needs:
  - dlrm_6_4_compact
  when: on_failure
dlrm_6_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - dlrm_6_4_move
  when: on_success
dlrm_6_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/dlrm/4/20250327140733
  needs:
    job: dlrm_6_4_compress_final
    optional: true
  when: always
dlrm_6_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=dlrm
    ++workload.dataset.data_folder=/tmp/dlrm//data ++workload.checkpoint.checkpoint_folder=/tmp/dlrm-6-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/dlrm/8/20250327140733/train
    hydra.run.dir=/tmp/dlrm/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/dlrm/8/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - dlrm_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
dlrm_6_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/dlrm/8/20250327140733/train
  - if find /tmp/dlrm/8/20250327140733/train -type f -name '*.pfw' | grep -q .; then
    echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/dlrm/8/20250327140733/train -type f -name '*.pfw.gz' | grep -q
    .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_8_train
dlrm_6_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/dlrm/nodes-8/20250327140733/RAW/
  - mv /tmp/dlrm/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/dlrm/nodes-8/20250327140733/RAW/
  - mv /tmp/dlrm/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/dlrm/nodes-8/20250327140733/
  - mv /tmp/dlrm/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/dlrm/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - dlrm_6_8_compress_output
dlrm_6_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n dlrm
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_8_move
dlrm_6_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/dlrm/nodes-8/20250327140733/COMPACT
  needs:
  - dlrm_6_8_compact
  when: on_failure
dlrm_6_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/dlrm/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - dlrm_6_8_move
  when: on_success
dlrm_6_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/dlrm/8/20250327140733
  needs:
    job: dlrm_6_8_compress_final
    optional: true
  when: always
llama_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/llama/ ] && [ ! -f /tmp/llama//success ]; then echo 'Directory /tmp/llama/
    exist but is not complete.'; flux run -N 1 --tasks-per-node=1 -q squeue -t 60
    --exclusive drm /tmp/llama/;  fi
  - if [ -f /tmp/llama//success ]; then echo 'Directory /tmp/llama/ already exists.
    Skipping data generation.'; else flux run -N 8 --tasks-per-node=1 -q squeue -t
    60 --exclusive dlio_benchmark workload=llama_70b_zero3 ++workload.dataset.data_folder=/tmp/llama//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_70b_zero3/8/20250327140733/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/llama/ ] && grep -i 'error' /tmp/llama_70b_zero3/8/20250327140733/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/llama//success
  needs:
  - create_directory_common
llama_70b_zero3_10_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_70b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_70b_zero3-10-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_70b_zero3/8/20250327140733/train
    hydra.run.dir=/tmp/llama_70b_zero3/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_70b_zero3/8/20250327140733/train/dlio.log; then
    echo 'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_70b_zero3_10_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_70b_zero3/8/20250327140733/train
  - if find /tmp/llama_70b_zero3/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_70b_zero3/8/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_70b_zero3_10_8_train
llama_70b_zero3_10_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_70b_zero3/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_70b_zero3/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_70b_zero3/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_70b_zero3/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_70b_zero3/nodes-8/20250327140733/
  - mv /tmp/llama_70b_zero3/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_70b_zero3/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - llama_70b_zero3_10_8_compress_output
llama_70b_zero3_10_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_70b_zero3/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_70b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_70b_zero3_10_8_move
llama_70b_zero3_10_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_70b_zero3/nodes-8/20250327140733/COMPACT
  needs:
  - llama_70b_zero3_10_8_compact
  when: on_failure
llama_70b_zero3_10_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_70b_zero3/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_70b_zero3_10_8_move
  when: on_success
llama_70b_zero3_10_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_70b_zero3/8/20250327140733
  needs:
    job: llama_70b_zero3_10_8_compress_final
    optional: true
  when: always
llama_7b_11_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b-11-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b/1/20250327140733/train
    hydra.run.dir=/tmp/llama_7b/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b/1/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_11_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b/1/20250327140733/train
  - if find /tmp/llama_7b/1/20250327140733/train -type f -name '*.pfw' | grep -q .;
    then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b/1/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_1_train
llama_7b_11_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b/nodes-1/20250327140733/RAW/
  - mv /tmp/llama_7b/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b/nodes-1/20250327140733/RAW/
  - mv /tmp/llama_7b/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b/nodes-1/20250327140733/
  - mv /tmp/llama_7b/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_11_1_compress_output
llama_7b_11_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_1_move
llama_7b_11_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b/nodes-1/20250327140733/COMPACT
  needs:
  - llama_7b_11_1_compact
  when: on_failure
llama_7b_11_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_11_1_move
  when: on_success
llama_7b_11_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b/1/20250327140733
  needs:
    job: llama_7b_11_1_compress_final
    optional: true
  when: always
llama_7b_11_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b-11-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b/2/20250327140733/train
    hydra.run.dir=/tmp/llama_7b/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b/2/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_11_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b/2/20250327140733/train
  - if find /tmp/llama_7b/2/20250327140733/train -type f -name '*.pfw' | grep -q .;
    then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b/2/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_2_train
llama_7b_11_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b/nodes-2/20250327140733/RAW/
  - mv /tmp/llama_7b/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b/nodes-2/20250327140733/RAW/
  - mv /tmp/llama_7b/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b/nodes-2/20250327140733/
  - mv /tmp/llama_7b/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_11_2_compress_output
llama_7b_11_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_2_move
llama_7b_11_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b/nodes-2/20250327140733/COMPACT
  needs:
  - llama_7b_11_2_compact
  when: on_failure
llama_7b_11_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_11_2_move
  when: on_success
llama_7b_11_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b/2/20250327140733
  needs:
    job: llama_7b_11_2_compress_final
    optional: true
  when: always
llama_7b_11_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b-11-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b/4/20250327140733/train
    hydra.run.dir=/tmp/llama_7b/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b/4/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_11_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b/4/20250327140733/train
  - if find /tmp/llama_7b/4/20250327140733/train -type f -name '*.pfw' | grep -q .;
    then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b/4/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_4_train
llama_7b_11_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b/nodes-4/20250327140733/RAW/
  - mv /tmp/llama_7b/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b/nodes-4/20250327140733/RAW/
  - mv /tmp/llama_7b/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b/nodes-4/20250327140733/
  - mv /tmp/llama_7b/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_11_4_compress_output
llama_7b_11_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_4_move
llama_7b_11_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b/nodes-4/20250327140733/COMPACT
  needs:
  - llama_7b_11_4_compact
  when: on_failure
llama_7b_11_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_11_4_move
  when: on_success
llama_7b_11_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b/4/20250327140733
  needs:
    job: llama_7b_11_4_compress_final
    optional: true
  when: always
llama_7b_11_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b-11-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b/8/20250327140733/train
    hydra.run.dir=/tmp/llama_7b/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b/8/20250327140733/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_11_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b/8/20250327140733/train
  - if find /tmp/llama_7b/8/20250327140733/train -type f -name '*.pfw' | grep -q .;
    then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b/8/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_8_train
llama_7b_11_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_7b/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_7b/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b/nodes-8/20250327140733/
  - mv /tmp/llama_7b/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_11_8_compress_output
llama_7b_11_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_8_move
llama_7b_11_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b/nodes-8/20250327140733/COMPACT
  needs:
  - llama_7b_11_8_compact
  when: on_failure
llama_7b_11_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_11_8_move
  when: on_success
llama_7b_11_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b/8/20250327140733
  needs:
    job: llama_7b_11_8_compress_final
    optional: true
  when: always
llama_7b_zero3_12_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b_zero3-12-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b_zero3/1/20250327140733/train
    hydra.run.dir=/tmp/llama_7b_zero3/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b_zero3/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_zero3_12_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b_zero3/1/20250327140733/train
  - if find /tmp/llama_7b_zero3/1/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b_zero3/1/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_1_train
llama_7b_zero3_12_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-1/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-1/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-1/20250327140733/
  - mv /tmp/llama_7b_zero3/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_zero3_12_1_compress_output
llama_7b_zero3_12_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_1_move
llama_7b_zero3_12_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-1/20250327140733/COMPACT
  needs:
  - llama_7b_zero3_12_1_compact
  when: on_failure
llama_7b_zero3_12_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_zero3_12_1_move
  when: on_success
llama_7b_zero3_12_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b_zero3/1/20250327140733
  needs:
    job: llama_7b_zero3_12_1_compress_final
    optional: true
  when: always
llama_7b_zero3_12_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b_zero3-12-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b_zero3/2/20250327140733/train
    hydra.run.dir=/tmp/llama_7b_zero3/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b_zero3/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_zero3_12_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b_zero3/2/20250327140733/train
  - if find /tmp/llama_7b_zero3/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b_zero3/2/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_2_train
llama_7b_zero3_12_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-2/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-2/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-2/20250327140733/
  - mv /tmp/llama_7b_zero3/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_zero3_12_2_compress_output
llama_7b_zero3_12_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_2_move
llama_7b_zero3_12_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-2/20250327140733/COMPACT
  needs:
  - llama_7b_zero3_12_2_compact
  when: on_failure
llama_7b_zero3_12_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_zero3_12_2_move
  when: on_success
llama_7b_zero3_12_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b_zero3/2/20250327140733
  needs:
    job: llama_7b_zero3_12_2_compress_final
    optional: true
  when: always
llama_7b_zero3_12_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b_zero3-12-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b_zero3/4/20250327140733/train
    hydra.run.dir=/tmp/llama_7b_zero3/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b_zero3/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_zero3_12_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b_zero3/4/20250327140733/train
  - if find /tmp/llama_7b_zero3/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b_zero3/4/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_4_train
llama_7b_zero3_12_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-4/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-4/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-4/20250327140733/
  - mv /tmp/llama_7b_zero3/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_zero3_12_4_compress_output
llama_7b_zero3_12_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_4_move
llama_7b_zero3_12_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-4/20250327140733/COMPACT
  needs:
  - llama_7b_zero3_12_4_compact
  when: on_failure
llama_7b_zero3_12_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_zero3_12_4_move
  when: on_success
llama_7b_zero3_12_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b_zero3/4/20250327140733
  needs:
    job: llama_7b_zero3_12_4_compress_final
    optional: true
  when: always
llama_7b_zero3_12_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_7b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b_zero3-12-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b_zero3/8/20250327140733/train
    hydra.run.dir=/tmp/llama_7b_zero3/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b_zero3/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_zero3_12_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_7b_zero3/8/20250327140733/train
  - if find /tmp/llama_7b_zero3/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b_zero3/8/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_8_train
llama_7b_zero3_12_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_7b_zero3/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-8/20250327140733/
  - mv /tmp/llama_7b_zero3/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - llama_7b_zero3_12_8_compress_output
llama_7b_zero3_12_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_8_move
llama_7b_zero3_12_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-8/20250327140733/COMPACT
  needs:
  - llama_7b_zero3_12_8_compact
  when: on_failure
llama_7b_zero3_12_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_7b_zero3/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_zero3_12_8_move
  when: on_success
llama_7b_zero3_12_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_7b_zero3/8/20250327140733
  needs:
    job: llama_7b_zero3_12_8_compress_final
    optional: true
  when: always
llama_8b_zero3_13_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_8b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_8b_zero3-13-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_8b_zero3/1/20250327140733/train
    hydra.run.dir=/tmp/llama_8b_zero3/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_8b_zero3/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_8b_zero3_13_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_8b_zero3/1/20250327140733/train
  - if find /tmp/llama_8b_zero3/1/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_8b_zero3/1/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_1_train
llama_8b_zero3_13_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-1/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-1/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-1/20250327140733/
  - mv /tmp/llama_8b_zero3/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - llama_8b_zero3_13_1_compress_output
llama_8b_zero3_13_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_8b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_1_move
llama_8b_zero3_13_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-1/20250327140733/COMPACT
  needs:
  - llama_8b_zero3_13_1_compact
  when: on_failure
llama_8b_zero3_13_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_8b_zero3_13_1_move
  when: on_success
llama_8b_zero3_13_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_8b_zero3/1/20250327140733
  needs:
    job: llama_8b_zero3_13_1_compress_final
    optional: true
  when: always
llama_8b_zero3_13_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_8b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_8b_zero3-13-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_8b_zero3/2/20250327140733/train
    hydra.run.dir=/tmp/llama_8b_zero3/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_8b_zero3/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_8b_zero3_13_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_8b_zero3/2/20250327140733/train
  - if find /tmp/llama_8b_zero3/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_8b_zero3/2/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_2_train
llama_8b_zero3_13_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-2/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-2/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-2/20250327140733/
  - mv /tmp/llama_8b_zero3/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - llama_8b_zero3_13_2_compress_output
llama_8b_zero3_13_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_8b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_2_move
llama_8b_zero3_13_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-2/20250327140733/COMPACT
  needs:
  - llama_8b_zero3_13_2_compact
  when: on_failure
llama_8b_zero3_13_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_8b_zero3_13_2_move
  when: on_success
llama_8b_zero3_13_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_8b_zero3/2/20250327140733
  needs:
    job: llama_8b_zero3_13_2_compress_final
    optional: true
  when: always
llama_8b_zero3_13_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_8b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_8b_zero3-13-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_8b_zero3/4/20250327140733/train
    hydra.run.dir=/tmp/llama_8b_zero3/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_8b_zero3/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_8b_zero3_13_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_8b_zero3/4/20250327140733/train
  - if find /tmp/llama_8b_zero3/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_8b_zero3/4/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_4_train
llama_8b_zero3_13_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-4/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-4/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-4/20250327140733/
  - mv /tmp/llama_8b_zero3/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - llama_8b_zero3_13_4_compress_output
llama_8b_zero3_13_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_8b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_4_move
llama_8b_zero3_13_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-4/20250327140733/COMPACT
  needs:
  - llama_8b_zero3_13_4_compact
  when: on_failure
llama_8b_zero3_13_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_8b_zero3_13_4_move
  when: on_success
llama_8b_zero3_13_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_8b_zero3/4/20250327140733
  needs:
    job: llama_8b_zero3_13_4_compress_final
    optional: true
  when: always
llama_8b_zero3_13_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=llama_8b_zero3
    ++workload.dataset.data_folder=/tmp/llama//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_8b_zero3-13-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_8b_zero3/8/20250327140733/train
    hydra.run.dir=/tmp/llama_8b_zero3/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_8b_zero3/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_8b_zero3_13_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/llama_8b_zero3/8/20250327140733/train
  - if find /tmp/llama_8b_zero3/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_8b_zero3/8/20250327140733/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_8_train
llama_8b_zero3_13_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-8/20250327140733/RAW/
  - mv /tmp/llama_8b_zero3/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-8/20250327140733/
  - mv /tmp/llama_8b_zero3/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - llama_8b_zero3_13_8_compress_output
llama_8b_zero3_13_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_8b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_8_move
llama_8b_zero3_13_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-8/20250327140733/COMPACT
  needs:
  - llama_8b_zero3_13_8_compact
  when: on_failure
llama_8b_zero3_13_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/llama_8b_zero3/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_8b_zero3_13_8_move
  when: on_success
llama_8b_zero3_13_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/llama_8b_zero3/8/20250327140733
  needs:
    job: llama_8b_zero3_13_8_compress_final
    optional: true
  when: always
resnet50_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/resnet50/ ] && [ ! -f /tmp/resnet50//success ]; then echo 'Directory
    /tmp/resnet50/ exist but is not complete.'; flux run -N 1 --tasks-per-node=1 -q
    squeue -t 60 --exclusive drm /tmp/resnet50/;  fi
  - if [ -f /tmp/resnet50//success ]; then echo 'Directory /tmp/resnet50/ already
    exists. Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue
    -t 60 --exclusive dlio_benchmark workload=resnet50_a100 ++workload.dataset.data_folder=/tmp/resnet50//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_a100/1/20250327140733/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/resnet50/ ] && grep -i 'error' /tmp/resnet50_a100/1/20250327140733/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/resnet50//success
  needs:
  - create_directory_common
resnet50_a100_15_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_a100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_a100-15-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_a100/1/20250327140733/train
    hydra.run.dir=/tmp/resnet50_a100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_a100/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_a100_15_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_a100/1/20250327140733/train
  - if find /tmp/resnet50_a100/1/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_a100/1/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_1_train
resnet50_a100_15_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-1/20250327140733/RAW/
  - mv /tmp/resnet50_a100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-1/20250327140733/RAW/
  - mv /tmp/resnet50_a100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-1/20250327140733/
  - mv /tmp/resnet50_a100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - resnet50_a100_15_1_compress_output
resnet50_a100_15_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_1_move
resnet50_a100_15_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-1/20250327140733/COMPACT
  needs:
  - resnet50_a100_15_1_compact
  when: on_failure
resnet50_a100_15_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_a100_15_1_move
  when: on_success
resnet50_a100_15_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_a100/1/20250327140733
  needs:
    job: resnet50_a100_15_1_compress_final
    optional: true
  when: always
resnet50_a100_15_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_a100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_a100-15-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_a100/2/20250327140733/train
    hydra.run.dir=/tmp/resnet50_a100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_a100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_a100_15_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_a100/2/20250327140733/train
  - if find /tmp/resnet50_a100/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_a100/2/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_2_train
resnet50_a100_15_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_a100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_a100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-2/20250327140733/
  - mv /tmp/resnet50_a100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - resnet50_a100_15_2_compress_output
resnet50_a100_15_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_2_move
resnet50_a100_15_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-2/20250327140733/COMPACT
  needs:
  - resnet50_a100_15_2_compact
  when: on_failure
resnet50_a100_15_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_a100_15_2_move
  when: on_success
resnet50_a100_15_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_a100/2/20250327140733
  needs:
    job: resnet50_a100_15_2_compress_final
    optional: true
  when: always
resnet50_a100_15_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_a100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_a100-15-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_a100/4/20250327140733/train
    hydra.run.dir=/tmp/resnet50_a100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_a100/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_a100_15_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_a100/4/20250327140733/train
  - if find /tmp/resnet50_a100/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_a100/4/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_4_train
resnet50_a100_15_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_a100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_a100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-4/20250327140733/
  - mv /tmp/resnet50_a100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - resnet50_a100_15_4_compress_output
resnet50_a100_15_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_4_move
resnet50_a100_15_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-4/20250327140733/COMPACT
  needs:
  - resnet50_a100_15_4_compact
  when: on_failure
resnet50_a100_15_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_a100_15_4_move
  when: on_success
resnet50_a100_15_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_a100/4/20250327140733
  needs:
    job: resnet50_a100_15_4_compress_final
    optional: true
  when: always
resnet50_a100_15_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_a100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_a100-15-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_a100/8/20250327140733/train
    hydra.run.dir=/tmp/resnet50_a100/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_a100/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_a100_15_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_a100/8/20250327140733/train
  - if find /tmp/resnet50_a100/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_a100/8/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_8_train
resnet50_a100_15_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_a100/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_a100/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-8/20250327140733/
  - mv /tmp/resnet50_a100/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - resnet50_a100_15_8_compress_output
resnet50_a100_15_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_8_move
resnet50_a100_15_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-8/20250327140733/COMPACT
  needs:
  - resnet50_a100_15_8_compact
  when: on_failure
resnet50_a100_15_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_a100/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_a100_15_8_move
  when: on_success
resnet50_a100_15_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_a100/8/20250327140733
  needs:
    job: resnet50_a100_15_8_compress_final
    optional: true
  when: always
resnet50_h100_16_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_h100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_h100-16-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_h100/1/20250327140733/train
    hydra.run.dir=/tmp/resnet50_h100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_h100/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_h100_16_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_h100/1/20250327140733/train
  - if find /tmp/resnet50_h100/1/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_h100/1/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_1_train
resnet50_h100_16_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-1/20250327140733/RAW/
  - mv /tmp/resnet50_h100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-1/20250327140733/RAW/
  - mv /tmp/resnet50_h100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-1/20250327140733/
  - mv /tmp/resnet50_h100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - resnet50_h100_16_1_compress_output
resnet50_h100_16_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_1_move
resnet50_h100_16_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-1/20250327140733/COMPACT
  needs:
  - resnet50_h100_16_1_compact
  when: on_failure
resnet50_h100_16_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_h100_16_1_move
  when: on_success
resnet50_h100_16_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_h100/1/20250327140733
  needs:
    job: resnet50_h100_16_1_compress_final
    optional: true
  when: always
resnet50_h100_16_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_h100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_h100-16-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_h100/2/20250327140733/train
    hydra.run.dir=/tmp/resnet50_h100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_h100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_h100_16_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_h100/2/20250327140733/train
  - if find /tmp/resnet50_h100/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_h100/2/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_2_train
resnet50_h100_16_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_h100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_h100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-2/20250327140733/
  - mv /tmp/resnet50_h100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - resnet50_h100_16_2_compress_output
resnet50_h100_16_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_2_move
resnet50_h100_16_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-2/20250327140733/COMPACT
  needs:
  - resnet50_h100_16_2_compact
  when: on_failure
resnet50_h100_16_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_h100_16_2_move
  when: on_success
resnet50_h100_16_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_h100/2/20250327140733
  needs:
    job: resnet50_h100_16_2_compress_final
    optional: true
  when: always
resnet50_h100_16_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_h100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_h100-16-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_h100/4/20250327140733/train
    hydra.run.dir=/tmp/resnet50_h100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_h100/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_h100_16_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_h100/4/20250327140733/train
  - if find /tmp/resnet50_h100/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_h100/4/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_4_train
resnet50_h100_16_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_h100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_h100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-4/20250327140733/
  - mv /tmp/resnet50_h100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - resnet50_h100_16_4_compress_output
resnet50_h100_16_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_4_move
resnet50_h100_16_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-4/20250327140733/COMPACT
  needs:
  - resnet50_h100_16_4_compact
  when: on_failure
resnet50_h100_16_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_h100_16_4_move
  when: on_success
resnet50_h100_16_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_h100/4/20250327140733
  needs:
    job: resnet50_h100_16_4_compress_final
    optional: true
  when: always
resnet50_h100_16_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_h100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_h100-16-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_h100/8/20250327140733/train
    hydra.run.dir=/tmp/resnet50_h100/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_h100/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_h100_16_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_h100/8/20250327140733/train
  - if find /tmp/resnet50_h100/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_h100/8/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_8_train
resnet50_h100_16_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_h100/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_h100/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-8/20250327140733/
  - mv /tmp/resnet50_h100/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - resnet50_h100_16_8_compress_output
resnet50_h100_16_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_8_move
resnet50_h100_16_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-8/20250327140733/COMPACT
  needs:
  - resnet50_h100_16_8_compact
  when: on_failure
resnet50_h100_16_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_h100/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_h100_16_8_move
  when: on_success
resnet50_h100_16_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_h100/8/20250327140733
  needs:
    job: resnet50_h100_16_8_compress_final
    optional: true
  when: always
resnet50_tf_17_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_tf
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_tf-17-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_tf/1/20250327140733/train
    hydra.run.dir=/tmp/resnet50_tf/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_tf/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_tf_17_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_tf/1/20250327140733/train
  - if find /tmp/resnet50_tf/1/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_tf/1/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_1_train
resnet50_tf_17_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-1/20250327140733/RAW/
  - mv /tmp/resnet50_tf/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-1/20250327140733/RAW/
  - mv /tmp/resnet50_tf/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-1/20250327140733/
  - mv /tmp/resnet50_tf/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - resnet50_tf_17_1_compress_output
resnet50_tf_17_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_tf
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_1_move
resnet50_tf_17_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-1/20250327140733/COMPACT
  needs:
  - resnet50_tf_17_1_compact
  when: on_failure
resnet50_tf_17_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_tf_17_1_move
  when: on_success
resnet50_tf_17_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_tf/1/20250327140733
  needs:
    job: resnet50_tf_17_1_compress_final
    optional: true
  when: always
resnet50_tf_17_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_tf
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_tf-17-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_tf/2/20250327140733/train
    hydra.run.dir=/tmp/resnet50_tf/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_tf/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_tf_17_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_tf/2/20250327140733/train
  - if find /tmp/resnet50_tf/2/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_tf/2/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_2_train
resnet50_tf_17_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_tf/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_tf/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-2/20250327140733/
  - mv /tmp/resnet50_tf/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - resnet50_tf_17_2_compress_output
resnet50_tf_17_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_tf
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_2_move
resnet50_tf_17_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-2/20250327140733/COMPACT
  needs:
  - resnet50_tf_17_2_compact
  when: on_failure
resnet50_tf_17_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_tf_17_2_move
  when: on_success
resnet50_tf_17_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_tf/2/20250327140733
  needs:
    job: resnet50_tf_17_2_compress_final
    optional: true
  when: always
resnet50_tf_17_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_tf
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_tf-17-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_tf/4/20250327140733/train
    hydra.run.dir=/tmp/resnet50_tf/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_tf/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_tf_17_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_tf/4/20250327140733/train
  - if find /tmp/resnet50_tf/4/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_tf/4/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_4_train
resnet50_tf_17_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_tf/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_tf/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-4/20250327140733/
  - mv /tmp/resnet50_tf/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - resnet50_tf_17_4_compress_output
resnet50_tf_17_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_tf
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_4_move
resnet50_tf_17_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-4/20250327140733/COMPACT
  needs:
  - resnet50_tf_17_4_compact
  when: on_failure
resnet50_tf_17_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_tf_17_4_move
  when: on_success
resnet50_tf_17_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_tf/4/20250327140733
  needs:
    job: resnet50_tf_17_4_compress_final
    optional: true
  when: always
resnet50_tf_17_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_tf
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_tf-17-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_tf/8/20250327140733/train
    hydra.run.dir=/tmp/resnet50_tf/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_tf/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_tf_17_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_tf/8/20250327140733/train
  - if find /tmp/resnet50_tf/8/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_tf/8/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_8_train
resnet50_tf_17_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_tf/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_tf/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-8/20250327140733/
  - mv /tmp/resnet50_tf/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - resnet50_tf_17_8_compress_output
resnet50_tf_17_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_tf
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_8_move
resnet50_tf_17_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-8/20250327140733/COMPACT
  needs:
  - resnet50_tf_17_8_compact
  when: on_failure
resnet50_tf_17_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_tf/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_tf_17_8_move
  when: on_success
resnet50_tf_17_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_tf/8/20250327140733
  needs:
    job: resnet50_tf_17_8_compress_final
    optional: true
  when: always
resnet50_v100_18_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_v100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_v100-18-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_v100/2/20250327140733/train
    hydra.run.dir=/tmp/resnet50_v100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_v100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_v100_18_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_v100/2/20250327140733/train
  - if find /tmp/resnet50_v100/2/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_v100/2/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_2_train
resnet50_v100_18_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_v100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/resnet50_v100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-2/20250327140733/
  - mv /tmp/resnet50_v100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - resnet50_v100_18_2_compress_output
resnet50_v100_18_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_2_move
resnet50_v100_18_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-2/20250327140733/COMPACT
  needs:
  - resnet50_v100_18_2_compact
  when: on_failure
resnet50_v100_18_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_v100_18_2_move
  when: on_success
resnet50_v100_18_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_v100/2/20250327140733
  needs:
    job: resnet50_v100_18_2_compress_final
    optional: true
  when: always
resnet50_v100_18_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_v100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_v100-18-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_v100/4/20250327140733/train
    hydra.run.dir=/tmp/resnet50_v100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_v100/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_v100_18_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_v100/4/20250327140733/train
  - if find /tmp/resnet50_v100/4/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_v100/4/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_4_train
resnet50_v100_18_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_v100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/resnet50_v100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-4/20250327140733/
  - mv /tmp/resnet50_v100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - resnet50_v100_18_4_compress_output
resnet50_v100_18_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_4_move
resnet50_v100_18_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-4/20250327140733/COMPACT
  needs:
  - resnet50_v100_18_4_compact
  when: on_failure
resnet50_v100_18_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_v100_18_4_move
  when: on_success
resnet50_v100_18_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_v100/4/20250327140733
  needs:
    job: resnet50_v100_18_4_compress_final
    optional: true
  when: always
resnet50_v100_18_8_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 8 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=resnet50_v100
    ++workload.dataset.data_folder=/tmp/resnet50//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_v100-18-8//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/resnet50_v100/8/20250327140733/train
    hydra.run.dir=/tmp/resnet50_v100/8/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_v100/8/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_v100_18_8_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/resnet50_v100/8/20250327140733/train
  - if find /tmp/resnet50_v100/8/20250327140733/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_v100/8/20250327140733/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_8_train
resnet50_v100_18_8_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_v100/8/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-8/20250327140733/RAW/
  - mv /tmp/resnet50_v100/8/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-8/20250327140733/
  - mv /tmp/resnet50_v100/8/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-8/20250327140733/
  needs:
  - create_directory_common
  - resnet50_v100_18_8_compress_output
resnet50_v100_18_8_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-8/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_8_move
resnet50_v100_18_8_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-8/20250327140733/COMPACT
  needs:
  - resnet50_v100_18_8_compact
  when: on_failure
resnet50_v100_18_8_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/resnet50_v100/nodes-8/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - resnet50_v100_18_8_move
  when: on_success
resnet50_v100_18_8_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/resnet50_v100/8/20250327140733
  needs:
    job: resnet50_v100_18_8_compress_final
    optional: true
  when: always
unet3d_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/unet3d/ ] && [ ! -f /tmp/unet3d//success ]; then echo 'Directory
    /tmp/unet3d/ exist but is not complete.'; flux run -N 1 --tasks-per-node=1 -q
    squeue -t 60 --exclusive drm /tmp/unet3d/;  fi
  - if [ -f /tmp/unet3d//success ]; then echo 'Directory /tmp/unet3d/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    60 --exclusive dlio_benchmark workload=unet3d_a100 ++workload.dataset.data_folder=/tmp/unet3d//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_a100/1/20250327140733/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/unet3d/ ] && grep -i 'error' /tmp/unet3d_a100/1/20250327140733/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/unet3d//success
  needs:
  - create_directory_common
unet3d_a100_19_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=unet3d_a100
    ++workload.dataset.data_folder=/tmp/unet3d//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_a100-19-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_a100/1/20250327140733/train
    hydra.run.dir=/tmp/unet3d_a100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_a100/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_a100_19_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/unet3d_a100/1/20250327140733/train
  - if find /tmp/unet3d_a100/1/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_a100/1/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_a100_19_1_train
unet3d_a100_19_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-1/20250327140733/RAW/
  - mv /tmp/unet3d_a100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-1/20250327140733/RAW/
  - mv /tmp/unet3d_a100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-1/20250327140733/
  - mv /tmp/unet3d_a100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - unet3d_a100_19_1_compress_output
unet3d_a100_19_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_a100_19_1_move
unet3d_a100_19_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-1/20250327140733/COMPACT
  needs:
  - unet3d_a100_19_1_compact
  when: on_failure
unet3d_a100_19_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_a100_19_1_move
  when: on_success
unet3d_a100_19_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/unet3d_a100/1/20250327140733
  needs:
    job: unet3d_a100_19_1_compress_final
    optional: true
  when: always
unet3d_a100_19_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=unet3d_a100
    ++workload.dataset.data_folder=/tmp/unet3d//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_a100-19-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_a100/2/20250327140733/train
    hydra.run.dir=/tmp/unet3d_a100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_a100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_a100_19_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/unet3d_a100/2/20250327140733/train
  - if find /tmp/unet3d_a100/2/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_a100/2/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_a100_19_2_train
unet3d_a100_19_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-2/20250327140733/RAW/
  - mv /tmp/unet3d_a100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-2/20250327140733/RAW/
  - mv /tmp/unet3d_a100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-2/20250327140733/
  - mv /tmp/unet3d_a100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - unet3d_a100_19_2_compress_output
unet3d_a100_19_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_a100_19_2_move
unet3d_a100_19_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-2/20250327140733/COMPACT
  needs:
  - unet3d_a100_19_2_compact
  when: on_failure
unet3d_a100_19_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/unet3d_a100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_a100_19_2_move
  when: on_success
unet3d_a100_19_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/unet3d_a100/2/20250327140733
  needs:
    job: unet3d_a100_19_2_compress_final
    optional: true
  when: always
unet3d_h100_20_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=unet3d_h100
    ++workload.dataset.data_folder=/tmp/unet3d//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_h100-20-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_h100/1/20250327140733/train
    hydra.run.dir=/tmp/unet3d_h100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_h100/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_h100_20_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/unet3d_h100/1/20250327140733/train
  - if find /tmp/unet3d_h100/1/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_h100/1/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_h100_20_1_train
unet3d_h100_20_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-1/20250327140733/RAW/
  - mv /tmp/unet3d_h100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-1/20250327140733/RAW/
  - mv /tmp/unet3d_h100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-1/20250327140733/
  - mv /tmp/unet3d_h100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - unet3d_h100_20_1_compress_output
unet3d_h100_20_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_h100_20_1_move
unet3d_h100_20_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-1/20250327140733/COMPACT
  needs:
  - unet3d_h100_20_1_compact
  when: on_failure
unet3d_h100_20_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_h100_20_1_move
  when: on_success
unet3d_h100_20_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/unet3d_h100/1/20250327140733
  needs:
    job: unet3d_h100_20_1_compress_final
    optional: true
  when: always
unet3d_h100_20_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=unet3d_h100
    ++workload.dataset.data_folder=/tmp/unet3d//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_h100-20-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_h100/2/20250327140733/train
    hydra.run.dir=/tmp/unet3d_h100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_h100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_h100_20_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/unet3d_h100/2/20250327140733/train
  - if find /tmp/unet3d_h100/2/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_h100/2/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_h100_20_2_train
unet3d_h100_20_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-2/20250327140733/RAW/
  - mv /tmp/unet3d_h100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-2/20250327140733/RAW/
  - mv /tmp/unet3d_h100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-2/20250327140733/
  - mv /tmp/unet3d_h100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - unet3d_h100_20_2_compress_output
unet3d_h100_20_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_h100_20_2_move
unet3d_h100_20_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-2/20250327140733/COMPACT
  needs:
  - unet3d_h100_20_2_compact
  when: on_failure
unet3d_h100_20_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/unet3d_h100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_h100_20_2_move
  when: on_success
unet3d_h100_20_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/unet3d_h100/2/20250327140733
  needs:
    job: unet3d_h100_20_2_compress_final
    optional: true
  when: always
unet3d_v100_21_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=unet3d_v100
    ++workload.dataset.data_folder=/tmp/unet3d//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_v100-21-1//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_v100/1/20250327140733/train
    hydra.run.dir=/tmp/unet3d_v100/1/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_v100/1/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_v100_21_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/unet3d_v100/1/20250327140733/train
  - if find /tmp/unet3d_v100/1/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_v100/1/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_1_train
unet3d_v100_21_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-1/20250327140733/RAW/
  - mv /tmp/unet3d_v100/1/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-1/20250327140733/RAW/
  - mv /tmp/unet3d_v100/1/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-1/20250327140733/
  - mv /tmp/unet3d_v100/1/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-1/20250327140733/
  needs:
  - create_directory_common
  - unet3d_v100_21_1_compress_output
unet3d_v100_21_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-1/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_1_move
unet3d_v100_21_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-1/20250327140733/COMPACT
  needs:
  - unet3d_v100_21_1_compact
  when: on_failure
unet3d_v100_21_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-1/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_v100_21_1_move
  when: on_success
unet3d_v100_21_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/unet3d_v100/1/20250327140733
  needs:
    job: unet3d_v100_21_1_compress_final
    optional: true
  when: always
unet3d_v100_21_2_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 2 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=unet3d_v100
    ++workload.dataset.data_folder=/tmp/unet3d//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_v100-21-2//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_v100/2/20250327140733/train
    hydra.run.dir=/tmp/unet3d_v100/2/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_v100/2/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_v100_21_2_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/unet3d_v100/2/20250327140733/train
  - if find /tmp/unet3d_v100/2/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_v100/2/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_2_train
unet3d_v100_21_2_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/unet3d_v100/2/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-2/20250327140733/RAW/
  - mv /tmp/unet3d_v100/2/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-2/20250327140733/
  - mv /tmp/unet3d_v100/2/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-2/20250327140733/
  needs:
  - create_directory_common
  - unet3d_v100_21_2_compress_output
unet3d_v100_21_2_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-2/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_2_move
unet3d_v100_21_2_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-2/20250327140733/COMPACT
  needs:
  - unet3d_v100_21_2_compact
  when: on_failure
unet3d_v100_21_2_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-2/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_v100_21_2_move
  when: on_success
unet3d_v100_21_2_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/unet3d_v100/2/20250327140733
  needs:
    job: unet3d_v100_21_2_compress_final
    optional: true
  when: always
unet3d_v100_21_4_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 4 --tasks-per-node=1 -q squeue -t 60 --exclusive dlio_benchmark workload=unet3d_v100
    ++workload.dataset.data_folder=/tmp/unet3d//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_v100-21-4//checkpoint
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_v100/4/20250327140733/train
    hydra.run.dir=/tmp/unet3d_v100/4/20250327140733/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_v100/4/20250327140733/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_v100_21_4_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_pgzip -d
    /tmp/unet3d_v100/4/20250327140733/train
  - if find /tmp/unet3d_v100/4/20250327140733/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_v100/4/20250327140733/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_4_train
unet3d_v100_21_4_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/unet3d_v100/4/20250327140733/train/*.pfw.gz /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-4/20250327140733/RAW/
  - mv /tmp/unet3d_v100/4/20250327140733/train/.hydra /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-4/20250327140733/
  - mv /tmp/unet3d_v100/4/20250327140733/train/dlio.log /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-4/20250327140733/
  needs:
  - create_directory_common
  - unet3d_v100_21_4_compress_output
unet3d_v100_21_4_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-4/20250327140733
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive dftracer_split -d
    $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_4_move
unet3d_v100_21_4_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-4/20250327140733/COMPACT
  needs:
  - unet3d_v100_21_4_compact
  when: on_failure
unet3d_v100_21_4_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev6/system/unet3d_v100/nodes-4/20250327140733
  - tar -czf RAW.tar.gz RAW
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_v100_21_4_move
  when: on_success
unet3d_v100_21_4_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 60 --exclusive drm /tmp/unet3d_v100/4/20250327140733
  needs:
    job: unet3d_v100_21_4_compress_final
    optional: true
  when: always
