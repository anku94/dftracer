variables: {}
stages:
- generate_data
- train
- compress_output
- create_directory
- move
- compact
- compress_final
- cleanup
include:
- project: lc-templates/id_tokens
  file: id_tokens.yml
- local: .gitlab/scripts/common.yml
bert_v100_1_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/bert-1/ ]; then echo 'Directory /tmp/bert-1/ already exists. Skipping
    data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive
    dlio_benchmark workload=bert_v100 ++workload.dataset.data_folder=/tmp/bert-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=313532 ++dataset.num_files_train=500
    ++workload.output.folder=/tmp/bert_v100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/bert-1/ ] && grep -i 'error' /tmp/bert_v100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
bert_v100_1_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=bert_v100
    ++workload.dataset.data_folder=/tmp/bert-1//data ++workload.checkpoint.checkpoint_folder=/tmp/bert_v100-1-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=313532 ++dataset.num_files_train=500
    ++workload.output.folder=/tmp/bert_v100/1/20250326112139/train hydra.run.dir=/tmp/bert_v100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/bert_v100/1/20250326112139/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - bert_v100_1_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
bert_v100_1_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/bert_v100/1/20250326112139/train
  - if find /tmp/bert_v100/1/20250326112139/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/bert_v100/1/20250326112139/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_1_train
bert_v100_1_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/bert_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/bert_v100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/bert_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/bert_v100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/bert_v100/nodes-1/20250326112139/
  - mv /tmp/bert_v100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/bert_v100/nodes-1/20250326112139/
  needs:
  - create_directory_common
bert_v100_1_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/bert_v100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n bert_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - bert_v100_1_1_move
bert_v100_1_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/bert_v100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - bert_v100_1_1_compact
bert_v100_1_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/bert_v100/1/20250326112139
  needs:
    job: bert_v100_1_1_compress_final
    optional: true
cosmoflow_a100_2_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/cosmoflow-1/ ]; then echo 'Directory /tmp/cosmoflow-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=cosmoflow_a100 ++workload.dataset.data_folder=/tmp/cosmoflow-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=388727
    ++workload.output.folder=/tmp/cosmoflow_a100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/cosmoflow-1/ ] && grep -i 'error' /tmp/cosmoflow_a100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
cosmoflow_a100_2_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=cosmoflow_a100
    ++workload.dataset.data_folder=/tmp/cosmoflow-1//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_a100-2-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=388727
    ++workload.output.folder=/tmp/cosmoflow_a100/1/20250326112139/train hydra.run.dir=/tmp/cosmoflow_a100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_a100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_a100_2_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_a100_2_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_a100/1/20250326112139/train
  - if find /tmp/cosmoflow_a100/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_a100/1/20250326112139/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_1_train
cosmoflow_a100_2_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/cosmoflow_a100/nodes-1/20250326112139/RAW/
  - mv /tmp/cosmoflow_a100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/cosmoflow_a100/nodes-1/20250326112139/RAW/
  - mv /tmp/cosmoflow_a100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/cosmoflow_a100/nodes-1/20250326112139/
  - mv /tmp/cosmoflow_a100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/cosmoflow_a100/nodes-1/20250326112139/
  needs:
  - create_directory_common
cosmoflow_a100_2_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/cosmoflow_a100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_a100_2_1_move
cosmoflow_a100_2_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/cosmoflow_a100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - cosmoflow_a100_2_1_compact
cosmoflow_a100_2_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/cosmoflow_a100/1/20250326112139
  needs:
    job: cosmoflow_a100_2_1_compress_final
    optional: true
cosmoflow_h100_3_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/cosmoflow-1/ ]; then echo 'Directory /tmp/cosmoflow-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=cosmoflow_h100 ++workload.dataset.data_folder=/tmp/cosmoflow-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=388727
    ++workload.output.folder=/tmp/cosmoflow_h100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/cosmoflow-1/ ] && grep -i 'error' /tmp/cosmoflow_h100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
cosmoflow_h100_3_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=cosmoflow_h100
    ++workload.dataset.data_folder=/tmp/cosmoflow-1//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_h100-3-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=388727
    ++workload.output.folder=/tmp/cosmoflow_h100/1/20250326112139/train hydra.run.dir=/tmp/cosmoflow_h100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_h100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_h100_3_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_h100_3_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_h100/1/20250326112139/train
  - if find /tmp/cosmoflow_h100/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_h100/1/20250326112139/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_1_train
cosmoflow_h100_3_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/cosmoflow_h100/nodes-1/20250326112139/RAW/
  - mv /tmp/cosmoflow_h100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/cosmoflow_h100/nodes-1/20250326112139/RAW/
  - mv /tmp/cosmoflow_h100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/cosmoflow_h100/nodes-1/20250326112139/
  - mv /tmp/cosmoflow_h100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/cosmoflow_h100/nodes-1/20250326112139/
  needs:
  - create_directory_common
cosmoflow_h100_3_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/cosmoflow_h100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_h100_3_1_move
cosmoflow_h100_3_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/cosmoflow_h100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - cosmoflow_h100_3_1_compact
cosmoflow_h100_3_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/cosmoflow_h100/1/20250326112139
  needs:
    job: cosmoflow_h100_3_1_compress_final
    optional: true
cosmoflow_v100_4_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/cosmoflow-1/ ]; then echo 'Directory /tmp/cosmoflow-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=cosmoflow_v100 ++workload.dataset.data_folder=/tmp/cosmoflow-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=388727
    ++workload.output.folder=/tmp/cosmoflow_v100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/cosmoflow-1/ ] && grep -i 'error' /tmp/cosmoflow_v100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
cosmoflow_v100_4_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=cosmoflow_v100
    ++workload.dataset.data_folder=/tmp/cosmoflow-1//data ++workload.checkpoint.checkpoint_folder=/tmp/cosmoflow_v100-4-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=388727
    ++workload.output.folder=/tmp/cosmoflow_v100/1/20250326112139/train hydra.run.dir=/tmp/cosmoflow_v100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/cosmoflow_v100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - cosmoflow_v100_4_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
cosmoflow_v100_4_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/cosmoflow_v100/1/20250326112139/train
  - if find /tmp/cosmoflow_v100/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/cosmoflow_v100/1/20250326112139/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_1_train
cosmoflow_v100_4_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/cosmoflow_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/cosmoflow_v100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/cosmoflow_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/cosmoflow_v100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/cosmoflow_v100/nodes-1/20250326112139/
  - mv /tmp/cosmoflow_v100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/cosmoflow_v100/nodes-1/20250326112139/
  needs:
  - create_directory_common
cosmoflow_v100_4_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/cosmoflow_v100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n cosmoflow_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - cosmoflow_v100_4_1_move
cosmoflow_v100_4_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/cosmoflow_v100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - cosmoflow_v100_4_1_compact
cosmoflow_v100_4_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/cosmoflow_v100/1/20250326112139
  needs:
    job: cosmoflow_v100_4_1_compress_final
    optional: true
dlrm_6_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/dlrm-1/ ]; then echo 'Directory /tmp/dlrm-1/ already exists. Skipping
    data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive
    dlio_benchmark workload=dlrm ++workload.dataset.data_folder=/tmp/dlrm-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=4195198976 ++dataset.num_files_train=0
    ++workload.output.folder=/tmp/dlrm/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/dlrm-1/ ] && grep -i 'error' /tmp/dlrm/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
dlrm_6_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=dlrm
    ++workload.dataset.data_folder=/tmp/dlrm-1//data ++workload.checkpoint.checkpoint_folder=/tmp/dlrm-6-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=4195198976 ++dataset.num_files_train=0
    ++workload.output.folder=/tmp/dlrm/1/20250326112139/train hydra.run.dir=/tmp/dlrm/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/dlrm/1/20250326112139/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - dlrm_6_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
dlrm_6_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/dlrm/1/20250326112139/train
  - if find /tmp/dlrm/1/20250326112139/train -type f -name '*.pfw' | grep -q .; then
    echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/dlrm/1/20250326112139/train -type f -name '*.pfw.gz' | grep -q
    .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_1_train
dlrm_6_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/dlrm/nodes-1/20250326112139/RAW/
  - mv /tmp/dlrm/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/dlrm/nodes-1/20250326112139/RAW/
  - mv /tmp/dlrm/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/dlrm/nodes-1/20250326112139/
  - mv /tmp/dlrm/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/dlrm/nodes-1/20250326112139/
  needs:
  - create_directory_common
dlrm_6_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/dlrm/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n dlrm
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - dlrm_6_1_move
dlrm_6_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/dlrm/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - dlrm_6_1_compact
dlrm_6_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/dlrm/1/20250326112139
  needs:
    job: dlrm_6_1_compress_final
    optional: true
llama_7b_11_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/llama-1/ ]; then echo 'Directory /tmp/llama-1/ already exists. Skipping
    data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive
    dlio_benchmark workload=llama_7b ++workload.dataset.data_folder=/tmp/llama-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1048576 ++dataset.num_files_train=1
    ++workload.output.folder=/tmp/llama_7b/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/llama-1/ ] && grep -i 'error' /tmp/llama_7b/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
llama_7b_11_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=llama_7b
    ++workload.dataset.data_folder=/tmp/llama-1//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b-11-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1048576 ++dataset.num_files_train=1
    ++workload.output.folder=/tmp/llama_7b/1/20250326112139/train hydra.run.dir=/tmp/llama_7b/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b/1/20250326112139/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - llama_7b_11_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_11_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/llama_7b/1/20250326112139/train
  - if find /tmp/llama_7b/1/20250326112139/train -type f -name '*.pfw' | grep -q .;
    then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b/1/20250326112139/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_1_train
llama_7b_11_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/llama_7b/nodes-1/20250326112139/RAW/
  - mv /tmp/llama_7b/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/llama_7b/nodes-1/20250326112139/RAW/
  - mv /tmp/llama_7b/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/llama_7b/nodes-1/20250326112139/
  - mv /tmp/llama_7b/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/llama_7b/nodes-1/20250326112139/
  needs:
  - create_directory_common
llama_7b_11_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/llama_7b/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_1_move
llama_7b_11_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/llama_7b/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - llama_7b_11_1_compact
llama_7b_11_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/llama_7b/1/20250326112139
  needs:
    job: llama_7b_11_1_compress_final
    optional: true
llama_7b_zero3_12_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/llama-1/ ]; then echo 'Directory /tmp/llama-1/ already exists. Skipping
    data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive
    dlio_benchmark workload=llama_7b_zero3 ++workload.dataset.data_folder=/tmp/llama-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1048576 ++dataset.num_files_train=1
    ++workload.output.folder=/tmp/llama_7b_zero3/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/llama-1/ ] && grep -i 'error' /tmp/llama_7b_zero3/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
llama_7b_zero3_12_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=llama_7b_zero3
    ++workload.dataset.data_folder=/tmp/llama-1//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b_zero3-12-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1048576 ++dataset.num_files_train=1
    ++workload.output.folder=/tmp/llama_7b_zero3/1/20250326112139/train hydra.run.dir=/tmp/llama_7b_zero3/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b_zero3/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_7b_zero3_12_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_zero3_12_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/llama_7b_zero3/1/20250326112139/train
  - if find /tmp/llama_7b_zero3/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b_zero3/1/20250326112139/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_1_train
llama_7b_zero3_12_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/llama_7b_zero3/nodes-1/20250326112139/RAW/
  - mv /tmp/llama_7b_zero3/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/llama_7b_zero3/nodes-1/20250326112139/RAW/
  - mv /tmp/llama_7b_zero3/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/llama_7b_zero3/nodes-1/20250326112139/
  - mv /tmp/llama_7b_zero3/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/llama_7b_zero3/nodes-1/20250326112139/
  needs:
  - create_directory_common
llama_7b_zero3_12_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/llama_7b_zero3/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_1_move
llama_7b_zero3_12_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/llama_7b_zero3/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - llama_7b_zero3_12_1_compact
llama_7b_zero3_12_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/llama_7b_zero3/1/20250326112139
  needs:
    job: llama_7b_zero3_12_1_compress_final
    optional: true
llama_8b_zero3_13_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/llama-1/ ]; then echo 'Directory /tmp/llama-1/ already exists. Skipping
    data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive
    dlio_benchmark workload=llama_8b_zero3 ++workload.dataset.data_folder=/tmp/llama-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1048576 ++dataset.num_files_train=1
    ++workload.output.folder=/tmp/llama_8b_zero3/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/llama-1/ ] && grep -i 'error' /tmp/llama_8b_zero3/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
llama_8b_zero3_13_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=llama_8b_zero3
    ++workload.dataset.data_folder=/tmp/llama-1//data ++workload.checkpoint.checkpoint_folder=/tmp/llama_8b_zero3-13-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1048576 ++dataset.num_files_train=1
    ++workload.output.folder=/tmp/llama_8b_zero3/1/20250326112139/train hydra.run.dir=/tmp/llama_8b_zero3/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_8b_zero3/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_8b_zero3_13_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_8b_zero3_13_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/llama_8b_zero3/1/20250326112139/train
  - if find /tmp/llama_8b_zero3/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_8b_zero3/1/20250326112139/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_1_train
llama_8b_zero3_13_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/llama_8b_zero3/nodes-1/20250326112139/RAW/
  - mv /tmp/llama_8b_zero3/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/llama_8b_zero3/nodes-1/20250326112139/RAW/
  - mv /tmp/llama_8b_zero3/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/llama_8b_zero3/nodes-1/20250326112139/
  - mv /tmp/llama_8b_zero3/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/llama_8b_zero3/nodes-1/20250326112139/
  needs:
  - create_directory_common
llama_8b_zero3_13_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/llama_8b_zero3/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_8b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_1_move
llama_8b_zero3_13_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/llama_8b_zero3/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - llama_8b_zero3_13_1_compact
llama_8b_zero3_13_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/llama_8b_zero3/1/20250326112139
  needs:
    job: llama_8b_zero3_13_1_compress_final
    optional: true
resnet50_a100_15_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/resnet50-1/ ]; then echo 'Directory /tmp/resnet50-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=resnet50_a100 ++workload.dataset.data_folder=/tmp/resnet50-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_a100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/resnet50-1/ ] && grep -i 'error' /tmp/resnet50_a100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
resnet50_a100_15_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=resnet50_a100
    ++workload.dataset.data_folder=/tmp/resnet50-1//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_a100-15-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_a100/1/20250326112139/train hydra.run.dir=/tmp/resnet50_a100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_a100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_a100_15_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_a100_15_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/resnet50_a100/1/20250326112139/train
  - if find /tmp/resnet50_a100/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_a100/1/20250326112139/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_1_train
resnet50_a100_15_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/resnet50_a100/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_a100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/resnet50_a100/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_a100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/resnet50_a100/nodes-1/20250326112139/
  - mv /tmp/resnet50_a100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/resnet50_a100/nodes-1/20250326112139/
  needs:
  - create_directory_common
resnet50_a100_15_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/resnet50_a100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_a100_15_1_move
resnet50_a100_15_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/resnet50_a100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - resnet50_a100_15_1_compact
resnet50_a100_15_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/resnet50_a100/1/20250326112139
  needs:
    job: resnet50_a100_15_1_compress_final
    optional: true
resnet50_h100_16_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/resnet50-1/ ]; then echo 'Directory /tmp/resnet50-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=resnet50_h100 ++workload.dataset.data_folder=/tmp/resnet50-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_h100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/resnet50-1/ ] && grep -i 'error' /tmp/resnet50_h100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
resnet50_h100_16_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=resnet50_h100
    ++workload.dataset.data_folder=/tmp/resnet50-1//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_h100-16-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_h100/1/20250326112139/train hydra.run.dir=/tmp/resnet50_h100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_h100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_h100_16_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_h100_16_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/resnet50_h100/1/20250326112139/train
  - if find /tmp/resnet50_h100/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_h100/1/20250326112139/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_1_train
resnet50_h100_16_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/resnet50_h100/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_h100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/resnet50_h100/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_h100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/resnet50_h100/nodes-1/20250326112139/
  - mv /tmp/resnet50_h100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/resnet50_h100/nodes-1/20250326112139/
  needs:
  - create_directory_common
resnet50_h100_16_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/resnet50_h100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_h100_16_1_move
resnet50_h100_16_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/resnet50_h100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - resnet50_h100_16_1_compact
resnet50_h100_16_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/resnet50_h100/1/20250326112139
  needs:
    job: resnet50_h100_16_1_compress_final
    optional: true
resnet50_tf_17_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/resnet50-1/ ]; then echo 'Directory /tmp/resnet50-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=resnet50_tf ++workload.dataset.data_folder=/tmp/resnet50-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_tf/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/resnet50-1/ ] && grep -i 'error' /tmp/resnet50_tf/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
resnet50_tf_17_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=resnet50_tf
    ++workload.dataset.data_folder=/tmp/resnet50-1//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_tf-17-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_tf/1/20250326112139/train hydra.run.dir=/tmp/resnet50_tf/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_tf/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_tf_17_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_tf_17_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/resnet50_tf/1/20250326112139/train
  - if find /tmp/resnet50_tf/1/20250326112139/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_tf/1/20250326112139/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_1_train
resnet50_tf_17_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/resnet50_tf/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_tf/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/resnet50_tf/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_tf/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/resnet50_tf/nodes-1/20250326112139/
  - mv /tmp/resnet50_tf/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/resnet50_tf/nodes-1/20250326112139/
  needs:
  - create_directory_common
resnet50_tf_17_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/resnet50_tf/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_tf
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_tf_17_1_move
resnet50_tf_17_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/resnet50_tf/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - resnet50_tf_17_1_compact
resnet50_tf_17_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/resnet50_tf/1/20250326112139
  needs:
    job: resnet50_tf_17_1_compress_final
    optional: true
resnet50_v100_18_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/resnet50-1/ ]; then echo 'Directory /tmp/resnet50-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=resnet50_v100 ++workload.dataset.data_folder=/tmp/resnet50-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_v100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/resnet50-1/ ] && grep -i 'error' /tmp/resnet50_v100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
resnet50_v100_18_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=resnet50_v100
    ++workload.dataset.data_folder=/tmp/resnet50-1//data ++workload.checkpoint.checkpoint_folder=/tmp/resnet50_v100-18-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1251 ++dataset.num_files_train=1024
    ++workload.output.folder=/tmp/resnet50_v100/1/20250326112139/train hydra.run.dir=/tmp/resnet50_v100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/resnet50_v100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - resnet50_v100_18_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
resnet50_v100_18_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/resnet50_v100/1/20250326112139/train
  - if find /tmp/resnet50_v100/1/20250326112139/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/resnet50_v100/1/20250326112139/train -type f -name '*.pfw.gz' |
    grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_1_train
resnet50_v100_18_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/resnet50_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_v100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/resnet50_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/resnet50_v100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/resnet50_v100/nodes-1/20250326112139/
  - mv /tmp/resnet50_v100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/resnet50_v100/nodes-1/20250326112139/
  needs:
  - create_directory_common
resnet50_v100_18_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/resnet50_v100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n resnet50_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - resnet50_v100_18_1_move
resnet50_v100_18_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/resnet50_v100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - resnet50_v100_18_1_compact
resnet50_v100_18_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/resnet50_v100/1/20250326112139
  needs:
    job: resnet50_v100_18_1_compress_final
    optional: true
unet3d_a100_19_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/unet3d-1/ ]; then echo 'Directory /tmp/unet3d-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=unet3d_a100 ++workload.dataset.data_folder=/tmp/unet3d-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=168
    ++workload.output.folder=/tmp/unet3d_a100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/unet3d-1/ ] && grep -i 'error' /tmp/unet3d_a100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
unet3d_a100_19_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=unet3d_a100
    ++workload.dataset.data_folder=/tmp/unet3d-1//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_a100-19-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=168
    ++workload.output.folder=/tmp/unet3d_a100/1/20250326112139/train hydra.run.dir=/tmp/unet3d_a100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_a100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_a100_19_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_a100_19_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/unet3d_a100/1/20250326112139/train
  - if find /tmp/unet3d_a100/1/20250326112139/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_a100/1/20250326112139/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_a100_19_1_train
unet3d_a100_19_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/unet3d_a100/nodes-1/20250326112139/RAW/
  - mv /tmp/unet3d_a100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/unet3d_a100/nodes-1/20250326112139/RAW/
  - mv /tmp/unet3d_a100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/unet3d_a100/nodes-1/20250326112139/
  - mv /tmp/unet3d_a100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/unet3d_a100/nodes-1/20250326112139/
  needs:
  - create_directory_common
unet3d_a100_19_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/unet3d_a100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_a100_19_1_move
unet3d_a100_19_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/unet3d_a100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - unet3d_a100_19_1_compact
unet3d_a100_19_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/unet3d_a100/1/20250326112139
  needs:
    job: unet3d_a100_19_1_compress_final
    optional: true
unet3d_h100_20_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/unet3d-1/ ]; then echo 'Directory /tmp/unet3d-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=unet3d_h100 ++workload.dataset.data_folder=/tmp/unet3d-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=168
    ++workload.output.folder=/tmp/unet3d_h100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/unet3d-1/ ] && grep -i 'error' /tmp/unet3d_h100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
unet3d_h100_20_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=unet3d_h100
    ++workload.dataset.data_folder=/tmp/unet3d-1//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_h100-20-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=168
    ++workload.output.folder=/tmp/unet3d_h100/1/20250326112139/train hydra.run.dir=/tmp/unet3d_h100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_h100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_h100_20_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_h100_20_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/unet3d_h100/1/20250326112139/train
  - if find /tmp/unet3d_h100/1/20250326112139/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_h100/1/20250326112139/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_h100_20_1_train
unet3d_h100_20_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/unet3d_h100/nodes-1/20250326112139/RAW/
  - mv /tmp/unet3d_h100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/unet3d_h100/nodes-1/20250326112139/RAW/
  - mv /tmp/unet3d_h100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/unet3d_h100/nodes-1/20250326112139/
  - mv /tmp/unet3d_h100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/unet3d_h100/nodes-1/20250326112139/
  needs:
  - create_directory_common
unet3d_h100_20_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/unet3d_h100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_h100_20_1_move
unet3d_h100_20_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/unet3d_h100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - unet3d_h100_20_1_compact
unet3d_h100_20_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/unet3d_h100/1/20250326112139
  needs:
    job: unet3d_h100_20_1_compress_final
    optional: true
unet3d_v100_21_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - if [ -d /tmp/unet3d-1/ ]; then echo 'Directory /tmp/unet3d-1/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive dlio_benchmark workload=unet3d_v100 ++workload.dataset.data_folder=/tmp/unet3d-1//data
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=168
    ++workload.output.folder=/tmp/unet3d_v100/1/20250326112139/generate ++workload.workflow.generate_data=True
    ++workload.workflow.train=False; fi
  - if [ -d /tmp/unet3d-1/ ] && grep -i 'error' /tmp/unet3d_v100/1/20250326112139/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
unet3d_v100_21_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dlio_benchmark workload=unet3d_v100
    ++workload.dataset.data_folder=/tmp/unet3d-1//data ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_v100-21-1//checkpoint
    ++workload.train.epochs=1 ++dataset.num_samples_per_file=1 ++dataset.num_files_train=168
    ++workload.output.folder=/tmp/unet3d_v100/1/20250326112139/train hydra.run.dir=/tmp/unet3d_v100/1/20250326112139/train
    ++workload.workflow.generate_data=False ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_v100/1/20250326112139/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_v100_21_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_v100_21_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive dftracer_pgzip -d
    /tmp/unet3d_v100/1/20250326112139/train
  - if find /tmp/unet3d_v100/1/20250326112139/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_v100/1/20250326112139/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_1_train
unet3d_v100_21_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev72/system/unet3d_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/unet3d_v100/1/20250326112139/train/*.pfw.gz /tmp/v1.0.10.dev72/system/unet3d_v100/nodes-1/20250326112139/RAW/
  - mv /tmp/unet3d_v100/1/20250326112139/train/.hydra /tmp/v1.0.10.dev72/system/unet3d_v100/nodes-1/20250326112139/
  - mv /tmp/unet3d_v100/1/20250326112139/train/dlio.log /tmp/v1.0.10.dev72/system/unet3d_v100/nodes-1/20250326112139/
  needs:
  - create_directory_common
unet3d_v100_21_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev72/system/unet3d_v100/nodes-1/20250326112139
  - dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_v100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_v100_21_1_move
unet3d_v100_21_1_compress_final:
  stage: compress_final
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - cd /tmp/v1.0.10.dev72/system/unet3d_v100/nodes-1/20250326112139
  - tar -czf RAW.tar.gz RAW
  - tar -czf COMPACT.tar.gz COMPACT
  needs:
  - unet3d_v100_21_1_compact
unet3d_v100_21_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/unet3d_v100/1/20250326112139
  needs:
    job: unet3d_v100_21_1_compress_final
    optional: true
create_directory_common:
  stage: create_directory
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - ./.gitlab/scripts/create_log_dir.sh
  needs:
  - job: bert_v100_1_1_compress_output
    optional: true
  - job: cosmoflow_a100_2_1_compress_output
    optional: true
  - job: cosmoflow_h100_3_1_compress_output
    optional: true
  - job: cosmoflow_v100_4_1_compress_output
    optional: true
  - job: dlrm_6_1_compress_output
    optional: true
  - job: llama_7b_11_1_compress_output
    optional: true
  - job: llama_7b_zero3_12_1_compress_output
    optional: true
  - job: llama_8b_zero3_13_1_compress_output
    optional: true
  - job: resnet50_a100_15_1_compress_output
    optional: true
  - job: resnet50_h100_16_1_compress_output
    optional: true
  - job: resnet50_tf_17_1_compress_output
    optional: true
  - job: resnet50_v100_18_1_compress_output
    optional: true
  - job: unet3d_a100_19_1_compress_output
    optional: true
  - job: unet3d_h100_20_1_compress_output
    optional: true
  - job: unet3d_v100_21_1_compress_output
    optional: true
