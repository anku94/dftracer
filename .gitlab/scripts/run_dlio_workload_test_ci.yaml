variables: {}
stages:
- create_directory
- generate_data
- train
- compress_output
- move
- create_summary
- compact
- cleanup_compact
- compress_final
- cleanup
include:
- project: lc-templates/id_tokens
  file: id_tokens.yml
- local: .gitlab/scripts/common.yml
create_directory_common:
  stage: create_directory
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - ./.gitlab/scripts/create_log_dir.sh
  needs: []
llama_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/llama/ ] && [ ! -f /tmp/llama//success ]; then echo 'Directory /tmp/llama/
    exists but is not complete.'; flux run -N 1 --tasks-per-node=1 -q squeue -t 1
    --exclusive --job-name llama_7b_drm drm /tmp/llama/;  fi
  - if [ -f /tmp/llama//success ]; then echo 'Directory /tmp/llama/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive --job-name llama_7b_gen dlio_benchmark workload=llama_7b ++workload.dataset.data_folder=/tmp/llama//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b/1/20250328104929/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/llama/ ] && grep -i 'error' /tmp/llama_7b/1/20250328104929/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/llama//success
  needs:
  - create_directory_common
llama_7b_11_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_train
    dlio_benchmark workload=llama_7b ++workload.dataset.data_folder=/tmp/llama//data
    ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b-11-1//checkpoint ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b/1/20250328104929/train
    hydra.run.dir=/tmp/llama_7b/1/20250328104929/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b/1/20250328104929/train/dlio.log; then echo 'Error
    found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_11_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_compress
    dftracer_pgzip -d /tmp/llama_7b/1/20250328104929/train
  - if find /tmp/llama_7b/1/20250328104929/train -type f -name '*.pfw' | grep -q .;
    then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b/1/20250328104929/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_11_1_train
llama_7b_11_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/RAW/
  - mv /tmp/llama_7b/1/20250328104929/train/*.pfw.gz /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/RAW/
  - mv /tmp/llama_7b/1/20250328104929/train/.hydra /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/
  - mv /tmp/llama_7b/1/20250328104929/train/dlio.log /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/
  - 'tar -czf /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/RAW.tar.gz
    /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/RAW '
  - rm -rf /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/RAW
  needs:
  - create_directory_common
  - llama_7b_11_1_compress_output
llama_7b_11_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_dfsplit
    dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_11_1_move
llama_7b_11_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_clean
    drm /tmp/v1.0.10.dev22/system/llama_7b/nodes-1/20250328104929/COMPACT
  needs:
  - llama_7b_11_1_compact
  when: on_failure
llama_7b_11_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/llama_7b/1/20250328104929
  needs:
    job: llama_7b_11_1_compress_final
    optional: true
  when: always
llama_7b_zero3_12_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_zero3_train
    dlio_benchmark workload=llama_7b_zero3 ++workload.dataset.data_folder=/tmp/llama//data
    ++workload.checkpoint.checkpoint_folder=/tmp/llama_7b_zero3-12-1//checkpoint ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_7b_zero3/1/20250328104929/train
    hydra.run.dir=/tmp/llama_7b_zero3/1/20250328104929/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_7b_zero3/1/20250328104929/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_7b_zero3_12_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_zero3_compress
    dftracer_pgzip -d /tmp/llama_7b_zero3/1/20250328104929/train
  - if find /tmp/llama_7b_zero3/1/20250328104929/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_7b_zero3/1/20250328104929/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_7b_zero3_12_1_train
llama_7b_zero3_12_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/RAW/
  - mv /tmp/llama_7b_zero3/1/20250328104929/train/*.pfw.gz /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/RAW/
  - mv /tmp/llama_7b_zero3/1/20250328104929/train/.hydra /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/
  - mv /tmp/llama_7b_zero3/1/20250328104929/train/dlio.log /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/
  - 'tar -czf /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/RAW.tar.gz
    /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/RAW '
  - rm -rf /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/RAW
  needs:
  - create_directory_common
  - llama_7b_zero3_12_1_compress_output
llama_7b_zero3_12_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_zero3_dfsplit
    dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_7b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_7b_zero3_12_1_move
llama_7b_zero3_12_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_7b_zero3_clean
    drm /tmp/v1.0.10.dev22/system/llama_7b_zero3/nodes-1/20250328104929/COMPACT
  needs:
  - llama_7b_zero3_12_1_compact
  when: on_failure
llama_7b_zero3_12_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/llama_7b_zero3/1/20250328104929
  needs:
    job: llama_7b_zero3_12_1_compress_final
    optional: true
  when: always
llama_8b_zero3_13_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_8b_zero3_train
    dlio_benchmark workload=llama_8b_zero3 ++workload.dataset.data_folder=/tmp/llama//data
    ++workload.checkpoint.checkpoint_folder=/tmp/llama_8b_zero3-13-1//checkpoint ++workload.train.epochs=1  ++workload.output.folder=/tmp/llama_8b_zero3/1/20250328104929/train
    hydra.run.dir=/tmp/llama_8b_zero3/1/20250328104929/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/llama_8b_zero3/1/20250328104929/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - llama_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
llama_8b_zero3_13_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_8b_zero3_compress
    dftracer_pgzip -d /tmp/llama_8b_zero3/1/20250328104929/train
  - if find /tmp/llama_8b_zero3/1/20250328104929/train -type f -name '*.pfw' | grep
    -q .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/llama_8b_zero3/1/20250328104929/train -type f -name '*.pfw.gz'
    | grep -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - llama_8b_zero3_13_1_train
llama_8b_zero3_13_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/RAW/
  - mv /tmp/llama_8b_zero3/1/20250328104929/train/*.pfw.gz /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/RAW/
  - mv /tmp/llama_8b_zero3/1/20250328104929/train/.hydra /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/
  - mv /tmp/llama_8b_zero3/1/20250328104929/train/dlio.log /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/
  - 'tar -czf /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/RAW.tar.gz
    /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/RAW '
  - rm -rf /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/RAW
  needs:
  - create_directory_common
  - llama_8b_zero3_13_1_compress_output
llama_8b_zero3_13_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_8b_zero3_dfsplit
    dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n llama_8b_zero3
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - llama_8b_zero3_13_1_move
llama_8b_zero3_13_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name llama_8b_zero3_clean
    drm /tmp/v1.0.10.dev22/system/llama_8b_zero3/nodes-1/20250328104929/COMPACT
  needs:
  - llama_8b_zero3_13_1_compact
  when: on_failure
llama_8b_zero3_13_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/llama_8b_zero3/1/20250328104929
  needs:
    job: llama_8b_zero3_13_1_compress_final
    optional: true
  when: always
unet3d_generate_data:
  stage: generate_data
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - export DLIO_LOG_LEVEL=info
  - module load mpifileutils
  - if [ -d /tmp/unet3d/ ] && [ ! -f /tmp/unet3d//success ]; then echo 'Directory
    /tmp/unet3d/ exists but is not complete.'; flux run -N 1 --tasks-per-node=1 -q
    squeue -t 1 --exclusive --job-name unet3d_a100_drm drm /tmp/unet3d/;  fi
  - if [ -f /tmp/unet3d//success ]; then echo 'Directory /tmp/unet3d/ already exists.
    Skipping data generation.'; else flux run -N 1 --tasks-per-node=1 -q squeue -t
    1 --exclusive --job-name unet3d_a100_gen dlio_benchmark workload=unet3d_a100 ++workload.dataset.data_folder=/tmp/unet3d//data
    ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_a100/1/20250328104929/generate
    ++workload.workflow.generate_data=True ++workload.workflow.train=False ++workload.workflow.checkpoint=False;
    fi
  - if [ -d /tmp/unet3d/ ] && grep -i 'error' /tmp/unet3d_a100/1/20250328104929/generate/dlio.log;
    then echo 'Error found in dlio.log'; exit 1; fi
  - touch /tmp/unet3d//success
  needs:
  - create_directory_common
unet3d_a100_19_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_a100_train
    dlio_benchmark workload=unet3d_a100 ++workload.dataset.data_folder=/tmp/unet3d//data
    ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_a100-19-1//checkpoint ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_a100/1/20250328104929/train
    hydra.run.dir=/tmp/unet3d_a100/1/20250328104929/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_a100/1/20250328104929/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_a100_19_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_a100_compress
    dftracer_pgzip -d /tmp/unet3d_a100/1/20250328104929/train
  - if find /tmp/unet3d_a100/1/20250328104929/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_a100/1/20250328104929/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_a100_19_1_train
unet3d_a100_19_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/RAW/
  - mv /tmp/unet3d_a100/1/20250328104929/train/*.pfw.gz /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/RAW/
  - mv /tmp/unet3d_a100/1/20250328104929/train/.hydra /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/
  - mv /tmp/unet3d_a100/1/20250328104929/train/dlio.log /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/
  - 'tar -czf /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/RAW.tar.gz
    /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/RAW '
  - rm -rf /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/RAW
  needs:
  - create_directory_common
  - unet3d_a100_19_1_compress_output
unet3d_a100_19_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_a100_dfsplit
    dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_a100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_a100_19_1_move
unet3d_a100_19_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_a100_clean
    drm /tmp/v1.0.10.dev22/system/unet3d_a100/nodes-1/20250328104929/COMPACT
  needs:
  - unet3d_a100_19_1_compact
  when: on_failure
unet3d_a100_19_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/unet3d_a100/1/20250328104929
  needs:
    job: unet3d_a100_19_1_compress_final
    optional: true
  when: always
unet3d_h100_20_1_train:
  stage: train
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dlio_benchmark;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_h100_train
    dlio_benchmark workload=unet3d_h100 ++workload.dataset.data_folder=/tmp/unet3d//data
    ++workload.checkpoint.checkpoint_folder=/tmp/unet3d_h100-20-1//checkpoint ++workload.train.epochs=1  ++workload.output.folder=/tmp/unet3d_h100/1/20250328104929/train
    hydra.run.dir=/tmp/unet3d_h100/1/20250328104929/train ++workload.workflow.generate_data=False
    ++workload.workflow.train=True
  - if grep -i 'error' /tmp/unet3d_h100/1/20250328104929/train/dlio.log; then echo
    'Error found in dlio.log'; exit 1; fi
  needs:
  - unet3d_generate_data
  variables:
    DFTRACER_ENABLE: '1'
    DFTRACER_INC_METADATA: '1'
unet3d_h100_20_1_compress_output:
  stage: compress_output
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_pgzip;
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_h100_compress
    dftracer_pgzip -d /tmp/unet3d_h100/1/20250328104929/train
  - if find /tmp/unet3d_h100/1/20250328104929/train -type f -name '*.pfw' | grep -q
    .; then echo 'Uncompressed .pfw files found!'; exit 1; fi
  - if ! find /tmp/unet3d_h100/1/20250328104929/train -type f -name '*.pfw.gz' | grep
    -q .; then echo 'No compressed .pfw.gz files found!'; exit 1; fi
  needs:
  - unet3d_h100_20_1_train
unet3d_h100_20_1_move:
  stage: move
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - mkdir -p /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/RAW/
  - mv /tmp/unet3d_h100/1/20250328104929/train/*.pfw.gz /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/RAW/
  - mv /tmp/unet3d_h100/1/20250328104929/train/.hydra /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/
  - mv /tmp/unet3d_h100/1/20250328104929/train/dlio.log /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/
  - 'tar -czf /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/RAW.tar.gz
    /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/RAW '
  - rm -rf /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/RAW
  needs:
  - create_directory_common
  - unet3d_h100_20_1_compress_output
unet3d_h100_20_1_compact:
  stage: compact
  extends: .system
  script:
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_split;
  - cd /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_h100_dfsplit
    dftracer_split -d $PWD/RAW -o $PWD/COMPACT -s 1024 -n unet3d_h100
  - if ! find $PWD/COMPACT -type f -name '*.pfw.gz' | grep -q .; then echo 'No compacted
    .pfw.gz files found!'; exit 1; fi
  - if [ -d COMPACT ]; then tar -czf COMPACT.tar.gz COMPACT; fi
  needs:
  - unet3d_h100_20_1_move
unet3d_h100_20_1_cleanup_compact:
  stage: cleanup_compact
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive --job-name unet3d_h100_clean
    drm /tmp/v1.0.10.dev22/system/unet3d_h100/nodes-1/20250328104929/COMPACT
  needs:
  - unet3d_h100_20_1_compact
  when: on_failure
unet3d_h100_20_1_cleanup:
  stage: cleanup
  extends: .system
  script:
  - module load mpifileutils
  - flux run -N 1 --tasks-per-node=1 -q squeue -t 1 --exclusive drm /tmp/unet3d_h100/1/20250328104929
  needs:
    job: unet3d_h100_20_1_compress_final
    optional: true
  when: always
create_summary:
  stage: create_summary
  extends: .system
  script:
  - ls
  - source .gitlab/scripts/variables.sh
  - source .gitlab/scripts/pre.sh
  - which python; which dftracer_event_count;
  - ./.gitlab/scripts/generate_summary.sh
  needs:
  - job: llama_7b_11_1_compact
    optional: true
  - job: llama_7b_zero3_12_1_compact
    optional: true
  - job: llama_8b_zero3_13_1_compact
    optional: true
  - job: unet3d_a100_19_1_compact
    optional: true
  - job: unet3d_h100_20_1_compact
    optional: true
