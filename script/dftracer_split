#!/bin/bash

# The script splits the traces into equal sized chunk optimized for analysis
# This has the following signature.
#
# usage: dftracer_split [-fv] [-n app_name] [-d input_directory] [-o output_directory] [-s chunk_size]
#   -n app_name             specify app name
#   -f                      override generated files
#   -s size                 chunk size (in MB)
#   -v                      enable verbose mode
#   -h                      display help
#   -d input_directory      specify input directories. should contain .pfw or .pfw.gz files.
#   -o output_directory     specify output directory

date_echo() {
  dt=$(date '+%d/%m/%Y %H:%M:%S')
  echo "$dt" "$@"
}

progress_date_echo() {
  dt=$(date '+%d/%m/%Y %H:%M:%S')
  echo -ne "$dt" "$@" "                              "\\r
}

LOG_DIR=$PWD
override=0
chunk_size=1024 # 1GB default
dest=$PWD/split
verbose=0
app_name="app"

function usage {
  echo "usage: $(basename "$0") [-fv] [-n app_name] [-d input_directory] [-o output_directory] [-s chunk_size]"
  echo "  -n app_name             specify app name"
  echo "  -f                      override generated files"
  echo "  -s size                 chunk size (in MB)"
  echo "  -v                      enable verbose mode"
  echo "  -h                      display help"
  echo "  -d input_directory      specify input directories. should contain .pfw or .pfw.gz files."
  echo "  -o output_directory     specify output directory"
  exit 1
}

# Parse arguments
while getopts ':fvn:d:o:s:h' opt; do
  case "$opt" in
  n)
    app_name="${OPTARG}"
    ;;
  d)
    LOG_DIR="${OPTARG}"
    ;;
  s)
    chunk_size="${OPTARG}"
    ;;
  o)
    dest="${OPTARG}"
    ;;
  f)
    override=1
    ;;
  v)
    verbose=1
    ;;
  h)
    usage
    ;;
  :)
    echo -e "option requires an argument.\n"
    usage
    ;;
  ?)
    echo -e "Invalid command option.\n"
    usage
    ;;
  esac
done

LOG_DIR=$(realpath "$LOG_DIR")
dest=$(realpath "$dest")

shift $((OPTIND - 1))

# Log initial arguments
date_echo "Starting script with the following arguments:"
date_echo "  App name: $app_name"
date_echo "  Override: $override"
date_echo "  Data dir: $LOG_DIR"
date_echo "  Output dir: $dest"
date_echo "  Chunk size: $chunk_size MB"

mkdir -p "$dest"

# Check for input files
pfw_total=$(find "$LOG_DIR" -type f -name "*.pfw" | wc -l)
pfw_gz_total=$(find "$LOG_DIR" -type f -name "*.pfw.gz" | wc -l)

if [ "$pfw_total" == "0" ] && [ "$pfw_gz_total" == "0" ]; then
  date_echo "The folder does not contain any .pfw or .pfw.gz files."
  exit 1
fi

# Check for zindex_py and zq
date_echo "Checking for zindex_py and zq..."
if ! python -c "import zindex_py;" &>/dev/null; then
  date_echo "zindex_py not found. Please install it using: pip install zindex_py"
  exit 1
fi
zindex_exec=$(python -c 'import zindex_py;import site; sp=site.getsitepackages()[0]; print(f"{sp}/zindex_py/bin/zindex")')
if [ ! -f "${zindex_exec}" ]; then
  date_echo "zindex executable not found. Please install zindex_py."
  exit 1
else
  date_echo "Found zindex executable at ${zindex_exec}"
fi

zq_exec=$(python -c 'import zindex_py;import site; sp=site.getsitepackages()[0]; print(f"{sp}/zindex_py/bin/zq")')
if [ ! -f "${zq_exec}" ]; then
  date_echo "zq executable not found. Please install zindex_py."
  exit 1
else
  date_echo "Found zq executable at ${zq_exec}"
fi

# Check for sqlite3
if command -v sqlite3 &> /dev/null; then
  date_echo "sqlite3 is available."
else
  date_echo "sqlite3 is not available. Will use Python for querying."
fi

# Create index
date_echo "Creating index for input files..."
SCRIPT_DIR="$(dirname "$(readlink -f "$0")")"
"$SCRIPT_DIR"/dftracer_create_index -c -d "$LOG_DIR" -f

# Start processing files
date_echo "Starting to process files..."
JOBS_LIMIT=1024
pushd "$LOG_DIR" > /dev/null || exit

files=("$LOG_DIR"/*.zindex)
total=${#files[@]}
declare -A count
counting_file="$LOG_DIR/counting.bak"
if [ ! -f "$counting_file" ] || [ $override == 1 ]; then
  date_echo "Collecting metadata for files..."
  rm -f "${counting_file}"
  touch "${counting_file}"
  for file_index in "${!files[@]}"; do
  running_jobs=$(jobs -rp | wc -l)
  if [ "$running_jobs" -ge $JOBS_LIMIT ]; then
    date_echo "Waiting for running jobs to drop below $JOBS_LIMIT..."
    while [ "$running_jobs" -ge $JOBS_LIMIT ]; do
      sleep 1
      running_jobs=$(jobs -rp | wc -l)
    done
  fi
  global_file_name=${files[$file_index]}
  filename_without_ext=$(basename "$global_file_name" .pfw.gz.zindex)
  (
    IFS='|' read -r local_size local_start local_end <<< "$(sqlite3 "$global_file_name" "select sum(length), min(line), max(line) as a from LineOffsets where length > 8;")"
    local_size_mb=$(bc -l <<< "scale=8; $local_size / (1024 * 1024)")
    echo "${file_index}|${filename_without_ext}|${local_size_mb}|${local_start}|${local_end}" >> "${counting_file}"
    progress_date_echo "Completed collecting size $local_size_mb for file $file_index of $total"
  ) &
  done
else
  date_echo "Previous metadata collection found. Skipping."
fi
wait

# Verify metadata collection
counts=$(wc -l "${counting_file}" | awk '{print $1}')
if [ "$counts" != "${#files[@]}" ]; then
  date_echo "Error: Metadata collection incomplete. Collected $counts of ${#files[@]} files."
  exit 1
else
  date_echo "Metadata collection complete for ${#files[@]} files."
fi

# Process chunks
date_echo "Starting chunk processing..."
chunk_index=1
file_processed=0
CHUNKS_LINES=()
while true; do
  chunk_file="$LOG_DIR/.chunk-${chunk_index}.bak"
  if [ -f "$chunk_file" ]; then
  files_in_chunk=$(wc -l "$chunk_file" | awk '{print $1}')
  chunk_index=$((chunk_index + 1))
  file_processed=$((file_processed + files_in_chunk))
  CHUNKS_LINES[${chunk_index}]=$file_processed
  else
  break
  fi
done

# Additional logging for chunk processing
date_echo "Processing chunks..."
# (Rest of the script continues with similar logging additions)
